{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Some Other Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the [DARIAH-DE tutorial on topic modeling with MALLET](https://de.dariah.eu/tatom/topic_model_mallet.html). Full explanations of the code it the underlying theory are available there. Note that this code was intended for use with small groups of novels, rather than large numbers of small documents. It may not run efficiently on our data, and the graphs may not be completely readable out of the box. **Consider this notebook experimental.**\n",
    "\n",
    "You should run this code after importing your data and running `2_clean_data.ipynb` on it. If you have a folder containing a prepared set of text files, you can also set that as your source directory in the configuration below.\n",
    "\n",
    "This notebook allows you to run MALLET and create your topic model independently of `3_make_topic_model.ipynb`. You do not need to run that notebook. However, if you have already created your model in that notebook, you can skip the \"Import Data to MALLET\" and \"Create Topic Model\" cells. The rest of the notebook should work.\n",
    "\n",
    "By default, this notebook runs its topic models with `--random-seed 1`. If you run the same model in `3_make_topic_model.ipynb` with this setting, you should get the same results.\n",
    "\n",
    "Some of the cells further down in the notebook can be take further configuration. Be sure to read any instructions above each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Name\n",
    "project_name = 'alternative_test'\n",
    "\n",
    "# Location of Source Files\n",
    "source_data = 'caches/text_files_clean' # Default\n",
    "# source_data = 'scrubbed' # Custom source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Project directory\n",
    "project_dir = %pwd\n",
    "print(project_dir)\n",
    "\n",
    "## Import project settings\n",
    "from settings import *\n",
    "\n",
    "## Make the model directory\n",
    "!mkdir -p {model_dir}\n",
    "\n",
    "## Set various variables\n",
    "CORPUS_PATH = os.path.join(project_dir, source_data)\n",
    "filenames = sorted([os.path.join(CORPUS_PATH, fn) for fn in os.listdir(CORPUS_PATH)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data to MALLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the mallet import command string\n",
    "mallet_import_args = '--input ' + CORPUS_PATH + ' ' \\\n",
    "  + '--output ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--keep-sequence ' \\\n",
    "  + '--remove-stopwords ' \\\n",
    "  + '--extra-stopwords ' + project_dir + '/' + stopwords_dir + '/' + stopwords_file + ' '\n",
    "mallet_import_command = 'mallet import-dir ' + mallet_import_args\n",
    "print(mallet_import_command+'\\n')\n",
    "\n",
    "## Run mallet; capture and display output\n",
    "mout = !mallet import-dir {mallet_import_args}\n",
    "print('\\n'.join(mout)+'\\n')\n",
    "\n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel import done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some helper variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set some helper variables\n",
    "_model_path = os.path.join(project_dir, model_dir)\n",
    "_model_file = os.path.join(_model_path, model_file)\n",
    "_model_state = os.path.join(_model_path, model_state)\n",
    "_output_doc_topics = os.path.join(_model_path, model_composition)\n",
    "_output_topic_keys = os.path.join(_model_path, model_keys)\n",
    "_word_topic_counts = os.path.join(_model_path, model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the mallet training command string\n",
    "mallet_train_args = '--input ' + _model_file + ' ' \\\n",
    "  + '--random-seed 1 ' \\\n",
    "  + '--num-topics ' + model_num_topics + ' ' \\\n",
    "  + '--optimize-interval 10 ' \\\n",
    "  + '--output-state ' + model_state + ' ' \\\n",
    "  + '--output-topic-keys ' + _output_topic_keys + ' ' \\\n",
    "  + '--output-doc-topics ' + _output_doc_topics + ' ' \\\n",
    "  + '--word-topic-counts-file ' + _word_topic_counts\n",
    "mallet_train_command = 'mallet train-topics ' + mallet_train_args\n",
    "print(mallet_train_command+'\\n')\n",
    "\n",
    "## Run mallet\n",
    "!mallet train-topics {mallet_train_args}\n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel training done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a DTM\n",
    "\n",
    "# Helper function\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue) \n",
    "\n",
    "doctopic_triples = []\n",
    "\n",
    "mallet_docnames = []\n",
    "\n",
    "with open(_output_doc_topics) as f:\n",
    "    f.readline()  # read one line in order to skip the header\n",
    "    for line in f:\n",
    "        docnum, docname, *values = line.rstrip().split('\\t')\n",
    "        mallet_docnames.append(docname)\n",
    "        for topic, share in grouper(2, values):\n",
    "            triple = (docname, int(topic), float(share))\n",
    "            doctopic_triples.append(triple)\n",
    "\n",
    "# Sort the triples\n",
    "# Triple is (docname, topicnum, share) so sort(key=operator.itemgetter(0,1))\n",
    "# Sorts on (docname, topicnum) which is what we want\n",
    "doctopic_triples = sorted(doctopic_triples, key=operator.itemgetter(0,1))\n",
    "\n",
    "# Sort the document names rather than relying on MALLET's ordering\n",
    "mallet_docnames = sorted(mallet_docnames)\n",
    "\n",
    "# Collect into a document-term matrix\n",
    "num_docs = len(mallet_docnames)\n",
    "\n",
    "num_topics = len(doctopic_triples) // len(mallet_docnames)\n",
    "\n",
    "doctopic = np.zeros((num_docs, num_topics))\n",
    "\n",
    "for i, (doc_name, triples) in enumerate(itertools.groupby(doctopic_triples, key=operator.itemgetter(0))):\n",
    "    doctopic[i, :] = np.array([share for _, _, share in triples])\n",
    "    \n",
    "print('Document-Term Matrix created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Article Names\n",
    "\n",
    "You have two options here. You can try to generate article names from the data file names or create a list of names yourself. The latter takes more work, but the former may create unwieldy or unhelpful article names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your own article names -- uncomment the next line to use this\n",
    "# article_names = ['article1', 'article2', 'article3']\n",
    "\n",
    "# Generate article names automatically -- comment out these lines to disable them\n",
    "article_names = []\n",
    "for fn in filenames:\n",
    "     basename = os.path.basename(fn)\n",
    "     name, ext = os.path.splitext(basename)\n",
    "     name = name.rstrip('.json')\n",
    "     article_names.append(name)\n",
    "    \n",
    "print('Article Names:')\n",
    "print(article_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Document-Topic Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get doc-topic groups\n",
    " \n",
    "# Turn article_names into an array so we can use NumPy functions\n",
    "article_names = np.asarray(article_names)\n",
    "\n",
    "doctopic_orig = doctopic.copy()\n",
    "\n",
    "# Group the doc-topic groups\n",
    "num_groups = len(set(article_names))\n",
    "\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "\n",
    "for i, name in enumerate(sorted(set(article_names))):\n",
    "     doctopic_grouped[i, :] = np.mean(doctopic[article_names == name, :], axis=0)\n",
    " \n",
    "doctopic = doctopic_grouped\n",
    "print('Doc-Topic groups array was generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Model for Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "filenames = [os.path.join(CORPUS_PATH, fn) for fn in sorted(os.listdir(CORPUS_PATH))]\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename')\n",
    "\n",
    "dtm = vectorizer.fit_transform(filenames)  # a sparse matrix\n",
    "\n",
    "dtm.shape\n",
    "\n",
    "dtm.data.nbytes  # number of bytes dtm takes up\n",
    "\n",
    "dtm.toarray().data.nbytes  # number of bytes dtm as array takes up\n",
    "\n",
    "doctopic_orig.shape\n",
    "\n",
    "doctopic_orig.data.nbytes  # number of bytes document-topic shares take up\n",
    "\n",
    "print('Document-Term Matrix prepared.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Significant Topics in Each Document\n",
    "\n",
    "Make sure that you configure the number of topics you wish to display for each topic in the `TOP_N_TOPICS` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify significant topics in each document\n",
    "\n",
    "# Number of topics to display\n",
    "TOP_N_TOPICS = 5\n",
    "\n",
    "articles = sorted(set(article_names))\n",
    "\n",
    "print(\"Top topics in...\\n\")\n",
    "\n",
    "for i in range(len(doctopic)):\n",
    "     top_topics = np.argsort(doctopic[i,:])[::-1][0:TOP_N_TOPICS]\n",
    "     top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "     print(\"{}: {}\".format(articles[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Topic-Word Distribution\n",
    "\n",
    "Configure the `N_WORDS_DISPLAY` variable below to change the number of terms displayed for each topic. If you wish to display the top words in a single topic, you can create a new cell and use `print(topic_words[1])` to print the words in topic 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic-word distribution\n",
    "\n",
    "N_WORDS_DISPLAY = 10\n",
    "\n",
    "with open(_output_topic_keys) as input:\n",
    "     topic_keys_lines = input.readlines()\n",
    "\n",
    "topic_words = []\n",
    "\n",
    "for line in topic_keys_lines:\n",
    "     _, _, words = line.split('\\t')  # tab-separated\n",
    "     words = words.rstrip().split(' ')  # remove the trailing '\\n'\n",
    "     topic_words.append(words)\n",
    "\n",
    "for t in range(len(topic_words)):\n",
    "     print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:N_WORDS_DISPLAY])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Distinctive Topics\n",
    "\n",
    "This tool allows you to determine which topics are most distinctive in a comparison of two documents. Make sure to configure `doc1` and `doc2` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the names of the two documents to be prepared (from the list of article_names)\n",
    "doc1          = \"guardian_hum_0_4\"\n",
    "doc2          = \"guardian_hum_118_3\"\n",
    "SHOW_N_TOPICS = 10\n",
    "\n",
    "# Find distinctive topics\n",
    "doc1_indices, doc2_indices = [], []\n",
    "\n",
    "for index, fn in enumerate(sorted(set(article_names))):\n",
    "     if doc1 in fn:\n",
    "         doc1_indices.append(index)\n",
    "     elif doc2 in fn:\n",
    "         doc2_indices.append(index)\n",
    "\n",
    "doc1_avg = np.mean(doctopic[doc1_indices, :], axis=0)\n",
    "\n",
    "doc2_avg = np.mean(doctopic[doc2_indices, :], axis=0)\n",
    "\n",
    "keyness = np.abs(doc1_avg - doc2_avg)\n",
    "\n",
    "ranking = np.argsort(keyness)[::-1]  # from highest to lowest; [::-1] reverses order in Python sequences\n",
    "\n",
    "# Show distinctive topics:\n",
    "print('Distinctive Topics (ranked from most to least):')\n",
    "print(ranking[:SHOW_N_TOPICS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Topic Shares in Layered Bar Chart\n",
    "\n",
    "By default, the image is 8\" x 8\". To adjust the size of the graph, modify the line `fig=plt.figure(figsize=(8, 8), dpi= 80)` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig=plt.figure(figsize=(8, 8), dpi= 80)\n",
    "\n",
    "# Layered bar chart\n",
    "N, K = doctopic.shape  # N documents, K topics\n",
    "\n",
    "ind = np.arange(N)  # the x-axis locations for the articles\n",
    "\n",
    "width = 0.5  # the width of the bars\n",
    "\n",
    "plots = []\n",
    "\n",
    "height_cumulative = np.zeros(N)\n",
    "\n",
    "for k in range(K):\n",
    "     color = plt.cm.coolwarm(k/K, 1)\n",
    "     if k == 0:\n",
    "         p = plt.bar(ind, doctopic[:, k], width, color=color)\n",
    "     else:\n",
    "         p = plt.bar(ind, doctopic[:, k], width, bottom=height_cumulative, color=color)\n",
    "     height_cumulative += doctopic[:, k]\n",
    "     plots.append(p)\n",
    "\n",
    "plt.ylim((0, 1))  # proportions sum to 1, so the height of the stacked bars is 1\n",
    "\n",
    "plt.ylabel('Topics')\n",
    "\n",
    "plt.title('Topics in articles')\n",
    "\n",
    "plt.xticks(ind+width/2, article_names)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.yticks(np.arange(0, 1, 10))\n",
    "\n",
    "topic_labels = ['Topic {}'.format(k) for k in range(K)]\n",
    "\n",
    "# see http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.legend for details\n",
    "# on making a legend in matplotlib\n",
    "# plt.legend([p[0] for p in plots], topic_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Topic Shares in Heatmap\n",
    "\n",
    "By default, the image is 15\" x 5\". To adjust the size of the graph, modify the line fig=plt.figure(figsize=(15, 5), dpi= 80) in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "fig=plt.figure(figsize=(15, 5), dpi= 80)\n",
    "\n",
    "# Ref: http://nbviewer.ipython.org/5427209\n",
    "# Ref: http://code.activestate.com/recipes/578175-hierarchical-clustering-heatmap-python/\n",
    "plt.pcolor(doctopic, norm=None, cmap='Blues')\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "# the trailing semicolon ';' suppresses output\n",
    "plt.yticks(np.arange(doctopic.shape[0])+0.5, article_names);\n",
    "\n",
    "plt.xticks(np.arange(doctopic.shape[1])+0.5, topic_labels);\n",
    "\n",
    "# flip the y-axis so the texts are in the order we anticipate\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# rotate the ticks on the x-axis\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# add a legend\n",
    "plt.colorbar(cmap='Blues')\n",
    "\n",
    "plt.tight_layout()  # fixes margins\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Topic-Word Associations\n",
    "\n",
    "Configure the number of topics and the number of top words to display below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-Word Associations\n",
    "\n",
    "num_topics = 50\n",
    "num_top_words = 10\n",
    "\n",
    "# Ignore this redundant code\n",
    "# with open(_output_topic_keys) as input:\n",
    "#      topic_keys_lines = input.readlines()\n",
    "\n",
    "# topic_words = []\n",
    "\n",
    "# for line in topic_keys_lines:\n",
    "#      _, _, words = line.split('\\t')  # tab-separated\n",
    "#      words = words.rstrip().split(' ')  # remove the trailing '\\n'\n",
    "#      topic_words.append(words)\n",
    "\n",
    "# for t in range(len(topic_words)):\n",
    "#      print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))\n",
    "\n",
    "\n",
    "mallet_vocab = []\n",
    "\n",
    "word_topic_counts = []\n",
    "\n",
    "with open(_word_topic_counts) as f:\n",
    "     for line in f:\n",
    "         _, word, *topic_count_pairs = line.rstrip().split(' ')\n",
    "         topic_count_pairs = [pair.split(':') for pair in topic_count_pairs]\n",
    "         mallet_vocab.append(word)\n",
    "         counts = np.zeros(num_topics)\n",
    "         for topic, count in topic_count_pairs:\n",
    "             counts[int(topic)] = int(count)\n",
    "         word_topic_counts.append(counts) \n",
    "\n",
    "word_topic = np.array(word_topic_counts)\n",
    "\n",
    "word_topic.shape\n",
    "    \n",
    "# np.sum(word_topic, axis=0) sums across rows, so it yields totals of words assigned to topics\n",
    "word_topic = word_topic / np.sum(word_topic, axis=0)\n",
    "\n",
    "mallet_vocab = np.array(mallet_vocab)  # convert vocab from a list to an array so we can use NumPy operations on it\n",
    "\n",
    "for t in range(num_topics):\n",
    "     top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "     top_words_idx = top_words_idx[:num_top_words]\n",
    "     top_words = mallet_vocab[top_words_idx]\n",
    "     top_words_shares = word_topic[top_words_idx, t]\n",
    "     print(\"Topic #{}:\".format(t))\n",
    "     for word, share in zip(top_words, top_words_shares):\n",
    "         print(\"{} : {}\".format(np.round(share, 3), word))\n",
    "\n",
    "# Word share visualisation -- experimental; probably won't work\n",
    "\n",
    "# num_top_words = 10\n",
    "\n",
    "# fontsize_base = 70 / np.max(word_topic) # font size for word with largest share in corpus\n",
    "\n",
    "# for t in range(num_topics):\n",
    "#      plt.subplot(1, num_topics, t + 1)  # plot numbering starts with 1\n",
    "#      plt.ylim(0, num_top_words + 0.5)  # stretch the y-axis to accommodate the words\n",
    "#      plt.xticks([])  # remove x-axis markings ('ticks')\n",
    "#      plt.yticks([]) # remove y-axis markings ('ticks')\n",
    "#      plt.title('Topic #{}'.format(t))\n",
    "#      top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "#      top_words_idx = top_words_idx[:num_top_words]\n",
    "#      top_words = mallet_vocab[top_words_idx]\n",
    "#      top_words_shares = word_topic[top_words_idx, t]\n",
    "#      for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "#          plt.text(0.3, num_top_words-i-0.5, word, fontsize=fontsize_base*share)\n",
    "\n",
    "# plt.tight_layout()\n",
    "    \n",
    "# # Number of word types associated with each topic\n",
    "# np.sum(word_topic > 0, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
