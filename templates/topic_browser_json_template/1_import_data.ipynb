{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TOPIC MODELING NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "## SETTINGS\n",
    "\n",
    "## project directory\n",
    "project_dir = %pwd\n",
    "print(project_dir)\n",
    "\n",
    "## import global project settings from config.py\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BROWSE: search zip filenames for keywords\n",
    "\n",
    "Choose search_text to filter available data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_text='mexico'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filespath = '/home/jovyan/work/data/'\n",
    "print(\"datafile_list = [\")\n",
    "for (dirname, _dirs, files) in os.walk(filespath):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.zip') and search_text in filename:\n",
    "            filepath = os.path.join(dirname.split(filespath)[1], filename)\n",
    "            print(\"    '\" + filepath + \"',\")\n",
    "print(\"                 ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST: define which zips will be used to import JSON files\n",
    "\n",
    "Optionally cut-paste the entire cell output and replace the datafile_list array in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondatadir = '/home/jovyan/work/data/data-new/'\n",
    "datafile_list = ['164282_deseretmorningnewssaltlakecity_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'164282_deseretmorningnews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1980-01-01_1980-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1981-01-01_1981-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1982-01-01_1982-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1983-01-01_1983-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1984-01-01_1984-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1985-01-01_1985-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1986-01-01_1986-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1987-01-01_1987-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1988-01-01_1988-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1989-01-01_1989-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1990-01-01_1990-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1991-01-01_1991-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1992-01-01_1992-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1993-01-01_1993-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1994-01-01_1994-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1995-01-01_1995-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1996-01-01_1996-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1997-01-01_1997-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1998-01-01_1998-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1999-01-01_1999-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2000-01-01_2000-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2001-01-01_2001-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2002-01-01_2002-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2003-01-01_2003-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2004-01-01_2004-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2005-01-01_2005-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2006-01-01_2006-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2007-01-01_2007-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2008-01-01_2008-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2009-01-01_2009-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2010-01-01_2010-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2011-01-01_2011-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2012-01-01_2012-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2013-01-01_2013-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2014-01-01_2014-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2015-01-01_2015-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2016-01-01_2016-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'256478_argusleader_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'247807_elnuevoherald_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'315698_jewishnews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'314477_jewishchronicle_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2005-01-01_2005-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2006-01-01_2006-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2007-01-01_2007-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2008-01-01_2008-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2009-01-01_2009-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2010-01-01_2010-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2011-01-01_2011-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2012-01-01_2012-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2013-01-01_2013-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2014-01-01_2014-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2015-01-01_2015-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2016-01-01_2016-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'152567_richmondtimesdispatch_bodypluralhumanities_2018-01-01_2018-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2010-01-01_2010-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2011-01-01_2011-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2012-01-01_2012-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2013-01-01_2013-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2014-01-01_2014-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2015-01-01_2015-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2016-01-01_2016-12-31.zip',\n",
    "'164263_bismarcktribune_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'163823_chicagodailyherald_bodypluralhumanitiesorhleadpluralhumanities_2016-01-01_2018-07-26.zip',\n",
    "'163823_chicagodailyherald_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'11810_stlouispostdispatch_bodysingularhumanity_2015-01-01_2015-12-31.zip',\n",
    "'11810_stlouispostdispatch_bodysingularhumanity_2016-01-01_2016-12-31.zip',\n",
    "'11810_stlouispostdispatch_bodysingularhumanity_2017-01-01_2017-12-31.zip',\n",
    "'11810_stlouispostdispatch_bodysingularhumanity_2018-01-01_2018-12-31.zip',\n",
    "'11810_stlouispostdispatch_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_1999-01-01_1999-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2000-01-01_2000-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2001-01-01_2001-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2002-01-01_2002-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2003-01-01_2003-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2004-01-01_2004-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2005-01-01_2005-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2006-01-01_2006-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2007-01-01_2007-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2008-01-01_2008-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2009-01-01_2009-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2010-01-01_2010-12-31.zip',\n",
    "'8206_theseattletimes_bodypluralhumanitiesorhleadpluralhumanities_2011-01-01_2011-12-31.zip',\n",
    "'8075_thewashingtonpost_bodypluralhumanitiesorhleadpluralhumanities_2016-01-01_2018-07-26.zip',\n",
    "'388137_thefilipinoreporter_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'279923_chicoenterpriserecord_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8006_losangelestimes_bodypluralhumanitiesorhleadpluralhumanities_2016-01-01_2018-07-26.zip',\n",
    "'8006_losangelestimes_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'387687_thejewishstarnewyork_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'144579_thesaltlaketribune_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8376_thehartfordcourant_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8113_newsdaynewyork_bodypluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8113_newsdaynewyork_bodypluralhumanities_2018-01-01_2018-12-31.zip',\n",
    "'417357_newsdayzimbabwe_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8113_newsday_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'144574_daytondailynews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'152788_tulsaworld_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'144553_thebaltimoresun_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'11811_thesandiegouniontribune_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'158440_thedailyoklahoman_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'147872_santafenewmexican_bodypluralhumanitiesorhleadpluralhumanities_2008-01-01_2008-12-31.zip',\n",
    "'147872_santafenewmexican_bodypluralhumanitiesorhleadpluralhumanities_2009-01-01_2009-12-31.zip',\n",
    "'147872_santafenewmexican_bodypluralhumanitiesorhleadpluralhumanities_2010-01-01_2010-12-31.zip',\n",
    "'147872_santafenewmexican_bodypluralhumanitiesorhleadpluralhumanities_2011-01-01_2011-12-31.zip',\n",
    "'147872_santafenewmexican_bodypluralhumanitiesorhleadpluralhumanities_2012-01-01_2012-12-31.zip',\n",
    "'144564_bangordailynews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'247189_thephiladelphiainquirer_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'313960_themercurynews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'350189_examinerwashingtondc_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'317177_dailycamera_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8384_startribune_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'11804_idahofallspostregister_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8379_theatlantajournalconstitution_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'157105_thedailynewsoflosangeles_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'151550_dailynewsnewyork_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'164207_newyorkpost_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8006_losangelestimes_bodypluralhumanitiesorhleadpluralhumanities_2016-01-01_2018-07-26.zip',\n",
    "'8006_losangelestimes_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'156942_theaugustachronicle_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2000-01-01_2000-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2001-01-01_2001-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2002-01-01_2002-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2004-01-01_2004-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2005-01-01_2005-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2006-01-01_2006-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2007-01-01_2007-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2008-01-01_2008-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2009-01-01_2009-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2010-01-01_2010-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2011-01-01_2011-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2012-01-01_2012-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2013-01-01_2013-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2014-01-01_2014-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2015-01-01_2015-12-31.zip',\n",
    "'145253_thedailyrecordandsundaymail_bodypluralhumanitiesorhleadpluralhumanities_2016-01-01_2016-12-31.zip',\n",
    "'148909_thedailyrecord_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'279927_eurekatimesstandardcalifornia_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'313961_stpaulpioneerpress_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'146155_crainsclevelandbusiness_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'255260_detroitnews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'374813_lasvegassun_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'145264_thecolumbian_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2107-12-31.zip',\n",
    "'317873_fifthestate_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'8204_investorsbusinessdaily_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'270098_theeveningsun_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'300814_theforward_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'438278_thefreepressfernie_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT: copy JSON from zip files to cache\n",
    "\n",
    "JSON files will be stored in the /caches/ project directory. Original zip source data remains untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "!rm -r caches/json\n",
    "!mkdir -p caches/json\n",
    "\n",
    "for datafile in datafile_list:\n",
    "    datapath = jsondatadir + datafile\n",
    "    !unzip -j -o -u \"{datapath}\" -d caches/json > /dev/null\n",
    "\n",
    "!ls caches/json | wc -l\n",
    "    \n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTER: delete non-matching JSON\n",
    "\n",
    "If you want to filter out any articles that do not contain a required keyword or phrase -- e.g. 'humanities' -- then write word here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_phrase = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the filter to delete JSON files that do not match. If no filter is defined, this step will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os, re, json\n",
    "\n",
    "if required_phrase:\n",
    "    \n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    del_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            json_content = json_decoded['content']\n",
    "            if not re.search(required_phrase, json_content, re.IGNORECASE):\n",
    "                os.remove(os.path.join(json_directory, filename))\n",
    "                del_count += 1\n",
    "                if(del_count%10==0):\n",
    "                    print('. ', end='')\n",
    "    new_num_docs = len(os.listdir(json_directory))\n",
    "    print('Number of documents deleted: ' + str(del_count))\n",
    "    print('Number of documents containing \"' + required_phrase + '\": ' + str(new_num_docs))\n",
    "else:\n",
    "    print('No required phrase, no documents deleted.')\n",
    "\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB: add scrubbed content to JSON\n",
    "\n",
    "Scrubbing is performed on each article JSON file, and the results are stored in a new key in the JSON file.\n",
    "\n",
    "-  To perform, set this step to True.\n",
    "-  If an article is already scrubbed it will be skipped unless rescrub is True.\n",
    "-  To reduce the JSON cache size, set delete original content. If original content is deleted then scrubbing cannot be repeated without re-exporting JSON from zip above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_scrub = True\n",
    "do_scrub_rescrub = True\n",
    "do_scrub_delete_original_content = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to scrub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "from scripts.scrub.scrub import scrub\n",
    "\n",
    "if do_scrub:\n",
    "\n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    scrub_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            if not 'content_scrubbed' in json_decoded or do_scrub_rescrub:\n",
    "                json_decoded['content_scrubbed'] = scrub(json_decoded['content'])\n",
    "                scrub_changed = True\n",
    "            if do_scrub_delete_original_content and 'content_scrubbed' in json_decoded:\n",
    "                json_decoded.pop('content', None)\n",
    "                scrub_changed = True\n",
    "        if scrub_changed:\n",
    "            with open(fpath, 'w') as json_file:\n",
    "                json.dump(json_decoded, json_file)\n",
    "            scrub_count += 1\n",
    "        ## progress indicator\n",
    "        if(scrub_count%100==0):\n",
    "            print('. ', end='')\n",
    "    print('Scrubbed ' + str(scrub_count) + ' files.')\n",
    "else:\n",
    "    print('Skipping scrub.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "**Deduplication is currently disabled, as it does not have an interface for large collections of JSON files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_dedupe = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "## For help on script options:\n",
    "## %run scripts/deduplicate/corpus_compare.py -h \n",
    "\n",
    "if do_dedupe:\n",
    "\n",
    "    print(project_dir)\n",
    "    print(dedup_dir)\n",
    "    print(dedup_name)\n",
    "    \n",
    "    ## delete previous results\n",
    "    !rm -f {dedup_dir}/{dedup_output}.csv\n",
    "    !rm -f {dedup_dir}/{dedup_output}.log\n",
    "    !rm -f {dedup_output}.log\n",
    "\n",
    "    !mkdir -p {text_files_clean_dir}\n",
    "    %run {dedup_dir}/{dedup} -i {text_files_clean_dir}/ -o {dedup_dir}/{dedup_name}.csv -l {dedup_dir}/{dedup_name}.log\n",
    "\n",
    "## --------------\n",
    "## FOR DockerFile\n",
    "## --------------\n",
    "## relies on sklearn\n",
    "## need to pip install or pip2 install or conda install scikit-learn?\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MERGE METADATA\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "if do_dedupe:\n",
    "    with open(project_dir + '/' + dedup_dir + '/' + dedup_name + '.csv','r') as fin:\n",
    "        cfin = csv.reader(fin)\n",
    "        # print(cfin, None)\n",
    "        next(cfin) # skip header\n",
    "        for row in cfin:\n",
    "            if os.path.isfile(row[5]):\n",
    "                print('Deleting: ' + row[5])\n",
    "                os.remove(row[5])\n",
    "            else:\n",
    "                print('Missing:  '+ row[5])\n",
    "    print('\\n-----\\nDuplicates deleted from:', dedup_dir + '/' + dedup_name + '.csv')\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT: MALLET text files and DFR csv metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## CREATE METADATA FROM JSON FILES\n",
    "\n",
    "import json\n",
    "\n",
    "## Delete old metadata files\n",
    "!rm -fr {metadata_dir}\n",
    "!mkdir -p {metadata_dir}\n",
    "\n",
    "## Delete old text files\n",
    "!rm -fr {text_files_dir}\n",
    "!mkdir -p {text_files_dir}\n",
    "\n",
    "json_directory = 'caches/json/'\n",
    "\n",
    "## DEFINE METADATA STRINGCLEANER\n",
    "\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "def string_cleaner(unistr):\n",
    "    \"\"\"Returns string in unaccented form, printable characters only.\"\"\"\n",
    "    unaccented = unidecode.unidecode(unistr)\n",
    "    printonly = ''.join(filter(lambda x:x in string.printable, unaccented))\n",
    "    return printonly\n",
    "\n",
    "## MAP FIELDS FROM JSON TO DFRB METADATA\n",
    "\n",
    "## id, publication, pubdate, title, articlebody, author, docUrl, wordcount\n",
    "\n",
    "## idx       ->  id\n",
    "## title     ->  title\n",
    "##           ->  author\n",
    "## pub       ->  publication\n",
    "##           ->  docUrl\n",
    "## length    ->  wordcount\n",
    "## pub_date  ->  pubdate\n",
    "\n",
    "## content   ->  articlebody\n",
    "\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "metadata_csv_file = 'caches/metadata/metadata-dfrb.csv'\n",
    "\n",
    "# ## infieldnames provides names for the original column order\n",
    "# infieldnames = 'id', 'publication', 'pubdate', 'title', 'articlebody', 'pagerange', 'author', 'docUrl', 'wordcount'\n",
    "# ## outfieldnames re-orders that name list into a new column order\n",
    "# outfieldnames = 'id', 'title', 'author', 'publication', 'docUrl', 'wordcount', 'pubdate', 'pagerange'\n",
    "\n",
    "\n",
    "with open(metadata_csv_file, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "#   csvwriter.writerow(['id'] + ['publication'] + ['pubdate'] + ['title'] + ['articlebody'] + ['author'] + ['docUrl'] + ['wordcount'])\n",
    "#   csvwriter.writerow(['id'] + ['title'] + ['author'] + ['publication'] + ['docUrl'] + ['wordcount'] + ['pubdate'] + ['pagerange'])\n",
    "    csvwriter.writerow(['id'] + ['title'] + ['author'] + ['journaltitle'] + ['volume'] + ['issue'] + ['pubdate'] + ['pagerange'])\n",
    "\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "    \n",
    "    idx = 0\n",
    "    for filename in sorted_json:\n",
    "\n",
    "        # log: preview the first and last files only to prevent log overflow\n",
    "        if(idx<5 or idx > len(sorted_json)-5):\n",
    "            print(idx, ':', filename, '\\n')\n",
    "        if(idx==5 and len(sorted_json)>10):\n",
    "            print('...\\n')\n",
    "            \n",
    "        with open(os.path.join(json_directory, filename)) as f:\n",
    "            j = json.loads(f.read())\n",
    "            if not 'pagerange' in j:\n",
    "                j['pagerange'] = 'no-pg'\n",
    "            if not 'author' in j:\n",
    "                j['author'] = 'unknown'\n",
    "            if not 'volume'in j:\n",
    "                j['volume'] = 'no-vol'\n",
    "            if not 'issue' in j:\n",
    "                j['issue'] = 'no-issue'\n",
    "\n",
    "            # write article metadata to csv\n",
    "            # csvwriter.writerow([idx] + [j['title']] + [] + [j['pub']] + [] + [j['length']] + [j['pub_date']])\n",
    "            csvwriter.writerow(['json/' + filename] + [j['title']] + [j['author']] + [j['pub']] + [j['volume']] + [j['issue']] + [j['pub_date']] + [j['length']])\n",
    "\n",
    "            # name article body file\n",
    "            padded_id = str(idx).zfill(len(str(len(sorted_json))))\n",
    "            \n",
    "            # write article body file to txt\n",
    "            with open(project_dir+'/' + text_files_clean_dir + '/'+ padded_id + '_.txt', 'w') as outfile:\n",
    "                if 'content_scrubbed' in j:\n",
    "                    outfile.write(string_cleaner(j['content_scrubbed']))\n",
    "                else:\n",
    "                    outfile.write(string_cleaner(j['content']))\n",
    "\n",
    "            idx = idx+1\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## CHECK METADATA\n",
    "\n",
    "!echo CHECK METADATA\n",
    "!echo\n",
    "!echo {metadata_dir} :\n",
    "!ls -1 {metadata_dir}\n",
    "!echo\n",
    "!echo {metadata_file_reorder} :\n",
    "!head -n 5 {metadata_file_reorder}\n",
    "!echo\n",
    "!echo CHECK TEXT FILES\n",
    "!echo\n",
    "!echo {text_files_clean_dir} :\n",
    "!ls -1 {text_files_clean_dir} | head\n",
    "!echo ...\n",
    "!ls -1 {text_files_clean_dir} | tail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL: build mallet topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## 1. run mallet -- import\n",
    "\n",
    "## build the mallet import command string\n",
    "mallet_import_args = '--input ' + project_dir + '/' + text_files_clean_dir + '/ ' \\\n",
    "  + '--output ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--keep-sequence ' \\\n",
    "  + '--remove-stopwords ' \\\n",
    "  + '--extra-stopwords ' + project_dir + '/' + stopwords_dir + '/' + stopwords_file + ' '\n",
    "mallet_import_command = 'mallet import-dir ' + mallet_import_args\n",
    "print(mallet_import_command+'\\n')\n",
    "\n",
    "## run mallet; capture and display output\n",
    "mout = !mallet import-dir {mallet_import_args}\n",
    "print('\\n'.join(mout)+'\\n')\n",
    "\n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel import done.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## 2. run mallet -- train\n",
    "\n",
    "## build the mallet training command string\n",
    "mallet_train_args = '--input ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--num-topics ' + model_num_topics + ' ' \\\n",
    "  + '--optimize-interval 10 ' \\\n",
    "  + '--output-state ' + project_dir + '/' + model_dir + '/' + model_state + ' ' \\\n",
    "  + '--output-topic-keys ' + project_dir + '/' + model_dir + '/' + model_keys + ' ' \\\n",
    "  + '--output-doc-topics ' + project_dir + '/' + model_dir + '/' + model_composition + ' ' \\\n",
    "  + '--word-topic-counts-file ' + project_dir + '/' + model_dir + '/' + model_counts\n",
    "if use_random_seed == True:\n",
    "  mallet_train_args += ' --random-seed ' + model_random_seed\n",
    "if generate_diagnostics == True:\n",
    "  mallet_train_args += ' --diagnostics-file ' + project_dir + '/' + model_dir + '/diagnostics.xml'\n",
    "    \n",
    "mallet_train_command = 'mallet train-topics ' + mallet_train_args\n",
    "print(mallet_train_command+'\\n')\n",
    "\n",
    "print('\\nRunning:\\n')\n",
    "\n",
    "## run mallet\n",
    "!mallet train-topics {mallet_train_args}\n",
    "    \n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel training done.')\n",
    "\n",
    "if generate_diagnostics == True:\n",
    "    print('A diagnostics web page will be generated soon. This feature is not yet active. In the meantime, you can view the diagnostics.xml file in your model directory.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NEXT\n",
    "## Generate a link to the next notebook in the workflow\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "browser_link_html = HTML('<p>The topic model is built.</p><h2>Make a <a href=\"4_make_topic_browser.ipynb\" target=\"_blank\"><strong>dfrbrowser</strong> topic browser</a></h2><p>...or...</p><h2>Make a <a href=\"6_browser_pyldavis.ipynb\" target=\"_blank\"><strong>pyLDAvis</strong> topic browser</a></h2>')\n",
    "display(browser_link_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
