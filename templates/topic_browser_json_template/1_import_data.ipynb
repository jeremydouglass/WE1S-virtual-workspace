{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TOPIC MODELING NOTEBOOK\n",
    "## Run top cell, then click its \"RUN ALL\" button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<button id=\"do_run_all\" style=\"font-size:40;\">RUN ALL</button>\n",
    "<script>\n",
    "$(\"#do_run_all\").click(\n",
    "    function () {\n",
    "        // assign port to Python variable\n",
    "        var port_command = \"port = \" + location.port + \"\";\n",
    "        IPython.notebook.kernel.execute(port_command);\n",
    "        // write notebook url bases for target ports\n",
    "        var url_parser = document.createElement(\"a\");\n",
    "        url_parser.href = location.href.substring(0, location.href.lastIndexOf(\"/\"));\n",
    "        url_parser.port = \"9999\";\n",
    "        var url_9999_command = \"url_9999 = '\" + url_parser.href + \"'\";\n",
    "        url_parser.port = \"10000\";\n",
    "        var url_10000_command = \"url_10000 = '\" + url_parser.href + \"'\";\n",
    "        // assign to Python variables\n",
    "        IPython.notebook.kernel.execute(url_9999_command);\n",
    "        IPython.notebook.kernel.execute(url_10000_command);\n",
    "        // in %%javascript cell only:\n",
    "        // element.html(port_command + '<br>' + url_9999_command + '<br>' + url_10000_command);\n",
    "        $(\"#run_all_cells\").click();\n",
    "    });\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "## SETTINGS\n",
    "\n",
    "## project directory\n",
    "project_dir = %pwd\n",
    "print(project_dir)\n",
    "\n",
    "## import global project settings from config.py\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect port / path / url environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// for manual running if not using run-all button\n",
    "\n",
    "// detect port\n",
    "var port = location.port;\n",
    "// assign to Python variable\n",
    "var port_command = \"port = \" + port + \"\";\n",
    "\n",
    "// write notebook url bases for target ports\n",
    "var url_parser = document.createElement(\"a\");\n",
    "url_parser.href = location.href.substring(0, location.href.lastIndexOf(\"/\"));\n",
    "url_parser.port = \"9999\";\n",
    "var url_9999_command = \"url_9999 = '\" + url_parser.href + \"'\";\n",
    "url_parser.port = \"10000\";\n",
    "var url_10000_command = \"url_10000 = '\" + url_parser.href + \"'\";\n",
    "\n",
    "// assign to Python variables\n",
    "IPython.notebook.kernel.execute(port_command);\n",
    "IPython.notebook.kernel.execute(url_9999_command);\n",
    "IPython.notebook.kernel.execute(url_10000_command);\n",
    "\n",
    "// display results\n",
    "element.html(port_command + '<br>' + url_9999_command + '<br>' + url_10000_command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(port)\n",
    "    print(url_9999)\n",
    "    print(url_10000)\n",
    "except NameError as e:\n",
    "    print(\"Not defined.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define non-url root path based on port\n",
    "jupter_root = \"/home/jovyan\"\n",
    "if (port==9999):\n",
    "    jupter_root = jupter_root + \"/work\"\n",
    "print('jupter_root =', jupter_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var url_parser = document.createElement(\"a\");\n",
    "url_parser.href = location.href;\n",
    "if(url_parser.port == \"10000\"){\n",
    "    url_parser.port = \"9999\";\n",
    "} else {\n",
    "    url_parser.port = \"10000\";\n",
    "}\n",
    "element.html('<p>If you wish, <strong>save first</strong> and then <a href=\"' + url_parser.href + '\"\"><strong>switch to this notebook on ' + url_parser.port + '</strong></a>.')\n",
    "\n",
    "// // assign to Python variable\n",
    "// IPython.notebook.kernel.execute(url_switch_command);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BROWSE: search zip filenames for keywords\n",
    "\n",
    "Choose search_text to filter available data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_text='mexico'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filespath = jupter_root + '/data/'\n",
    "print(\"datafile_list = [\")\n",
    "for (dirname, _dirs, files) in os.walk(filespath):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.zip') and search_text in filename:\n",
    "            filepath = os.path.join(dirname.split(filespath)[1], filename)\n",
    "            print(\"    '\" + filepath + \"',\")\n",
    "print(\"                 ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST: define which zips will be used to import JSON files\n",
    "\n",
    "Optionally cut-paste the entire cell output and replace the datafile_list array in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondatadir = jupter_root + '/data/data-new/'\n",
    "datafile_list = ['164282_deseretmorningnewssaltlakecity_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1980-01-01_1980-12-31.zip',\n",
    "'164282_deseretmorningnews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'300814_theforward_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'438278_thefreepressfernie_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT: copy JSON from zip files to cache\n",
    "\n",
    "JSON files will be stored in the /caches/ project directory. Original zip source data remains untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "!rm -r caches/json\n",
    "!mkdir -p caches/json\n",
    "\n",
    "for datafile in datafile_list:\n",
    "    datapath = jsondatadir + datafile\n",
    "    !unzip -j -o -u \"{datapath}\" -d caches/json > /dev/null\n",
    "\n",
    "!ls caches/json | wc -l\n",
    "    \n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTER: delete non-matching JSON\n",
    "\n",
    "If you want to filter out any articles that do not contain a required keyword or phrase -- e.g. 'humanities' -- then write word here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_phrase = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the filter to delete JSON files that do not match. If no filter is defined, this step will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os, re, json\n",
    "\n",
    "if required_phrase:\n",
    "    \n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    del_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            json_content = json_decoded['content']\n",
    "            if not re.search(required_phrase, json_content, re.IGNORECASE):\n",
    "                os.remove(os.path.join(json_directory, filename))\n",
    "                del_count += 1\n",
    "                if(del_count%10==0):\n",
    "                    print('. ', end='')\n",
    "    new_num_docs = len(os.listdir(json_directory))\n",
    "    print('Number of documents deleted: ' + str(del_count))\n",
    "    print('Number of documents containing \"' + required_phrase + '\": ' + str(new_num_docs))\n",
    "else:\n",
    "    print('No required phrase, no documents deleted.')\n",
    "\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB: add scrubbed content to JSON\n",
    "\n",
    "Scrubbing is performed on each article JSON file, and the results are stored in a new key in the JSON file.\n",
    "\n",
    "-  To perform, set this step to True.\n",
    "-  If an article is already scrubbed it will be skipped unless rescrub is True.\n",
    "-  To reduce the JSON cache size, set delete original content. If original content is deleted then scrubbing cannot be repeated without re-exporting JSON from zip above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_scrub = True\n",
    "do_scrub_rescrub = False\n",
    "do_scrub_delete_original_content = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to scrub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "from scripts.scrub.scrub import scrub\n",
    "\n",
    "if do_scrub:\n",
    "\n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    scrub_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            if 'content' in json_decoded and (not 'content_scrubbed' in json_decoded or do_scrub_rescrub):\n",
    "                json_decoded['content_scrubbed'] = scrub(json_decoded['content'])\n",
    "                scrub_changed = True\n",
    "            if do_scrub_delete_original_content and 'content_scrubbed' in json_decoded and 'content' in json_decoded:\n",
    "                json_decoded.pop('content', None)\n",
    "                scrub_changed = True\n",
    "        if scrub_changed:\n",
    "            with open(fpath, 'w') as json_file:\n",
    "                json.dump(json_decoded, json_file)\n",
    "            scrub_count += 1\n",
    "            ## progress indicator\n",
    "            if(scrub_count%100==0):\n",
    "                print('. ', end='')\n",
    "    print('Scrubbed ' + str(scrub_count) + ' files.')\n",
    "else:\n",
    "    print('Skipping scrub.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "**Deduplication is currently disabled, as it does not have an interface for large collections of JSON files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_dedupe = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "## For help on script options:\n",
    "## %run scripts/deduplicate/corpus_compare.py -h \n",
    "\n",
    "if do_dedupe:\n",
    "\n",
    "    print(project_dir)\n",
    "    print(dedup_dir)\n",
    "    print(dedup_name)\n",
    "    \n",
    "    ## delete previous results\n",
    "    !rm -f {dedup_dir}/{dedup_output}.csv\n",
    "    !rm -f {dedup_dir}/{dedup_output}.log\n",
    "    !rm -f {dedup_output}.log\n",
    "\n",
    "    !mkdir -p {text_files_clean_dir}\n",
    "    %run {dedup_dir}/{dedup} -i {text_files_clean_dir}/ -o {dedup_dir}/{dedup_name}.csv -l {dedup_dir}/{dedup_name}.log\n",
    "\n",
    "## --------------\n",
    "## FOR DockerFile\n",
    "## --------------\n",
    "## relies on sklearn\n",
    "## need to pip install or pip2 install or conda install scikit-learn?\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MERGE METADATA\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "if do_dedupe:\n",
    "    with open(project_dir + '/' + dedup_dir + '/' + dedup_name + '.csv','r') as fin:\n",
    "        cfin = csv.reader(fin)\n",
    "        # print(cfin, None)\n",
    "        next(cfin) # skip header\n",
    "        for row in cfin:\n",
    "            if os.path.isfile(row[5]):\n",
    "                print('Deleting: ' + row[5])\n",
    "                os.remove(row[5])\n",
    "            else:\n",
    "                print('Missing:  '+ row[5])\n",
    "    print('\\n-----\\nDuplicates deleted from:', dedup_dir + '/' + dedup_name + '.csv')\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT: MALLET text files and DFR csv metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## CREATE METADATA FROM JSON FILES\n",
    "\n",
    "import json\n",
    "\n",
    "## Delete old metadata files\n",
    "!rm -fr {metadata_dir}\n",
    "!mkdir -p {metadata_dir}\n",
    "\n",
    "## Delete old text files\n",
    "!rm -fr {text_files_clean_dir}\n",
    "!mkdir -p {text_files_clean_dir}\n",
    "\n",
    "json_directory = 'caches/json/'\n",
    "\n",
    "## DEFINE METADATA STRINGCLEANER\n",
    "\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "def string_cleaner(unistr):\n",
    "    \"\"\"Returns string in unaccented form, printable characters only.\"\"\"\n",
    "    unaccented = unidecode.unidecode(unistr)\n",
    "    printonly = ''.join(filter(lambda x:x in string.printable, unaccented))\n",
    "    return printonly\n",
    "\n",
    "## MAP FIELDS FROM JSON TO DFRB METADATA\n",
    "\n",
    "## id, publication, pubdate, title, articlebody, author, docUrl, wordcount\n",
    "\n",
    "## idx       ->  id\n",
    "## title     ->  title\n",
    "##           ->  author\n",
    "## pub       ->  publication\n",
    "##           ->  docUrl\n",
    "## length    ->  wordcount\n",
    "## pub_date  ->  pubdate\n",
    "\n",
    "## content   ->  articlebody\n",
    "\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "metadata_csv_file = 'caches/metadata/metadata-dfrb.csv'\n",
    "\n",
    "# ## infieldnames provides names for the original column order\n",
    "# infieldnames = 'id', 'publication', 'pubdate', 'title', 'articlebody', 'pagerange', 'author', 'docUrl', 'wordcount'\n",
    "# ## outfieldnames re-orders that name list into a new column order\n",
    "# outfieldnames = 'id', 'title', 'author', 'publication', 'docUrl', 'wordcount', 'pubdate', 'pagerange'\n",
    "\n",
    "\n",
    "with open(metadata_csv_file, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "#   csvwriter.writerow(['id'] + ['publication'] + ['pubdate'] + ['title'] + ['articlebody'] + ['author'] + ['docUrl'] + ['wordcount'])\n",
    "#   csvwriter.writerow(['id'] + ['title'] + ['author'] + ['publication'] + ['docUrl'] + ['wordcount'] + ['pubdate'] + ['pagerange'])\n",
    "    csvwriter.writerow(['id'] + ['title'] + ['author'] + ['journaltitle'] + ['volume'] + ['issue'] + ['pubdate'] + ['pagerange'])\n",
    "\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "    \n",
    "    idx = 0\n",
    "    for filename in sorted_json:\n",
    "\n",
    "        # log: preview the first and last files only to prevent log overflow\n",
    "        if(idx<5 or idx > len(sorted_json)-5):\n",
    "            print(idx, ':', filename, '\\n')\n",
    "        if(idx==5 and len(sorted_json)>10):\n",
    "            print('...\\n')\n",
    "            \n",
    "        with open(os.path.join(json_directory, filename)) as f:\n",
    "            j = json.loads(f.read())\n",
    "            if not 'pagerange' in j:\n",
    "                j['pagerange'] = 'no-pg'\n",
    "            if not 'author' in j:\n",
    "                j['author'] = 'unknown'\n",
    "            if not 'volume'in j:\n",
    "                j['volume'] = 'no-vol'\n",
    "            if not 'issue' in j:\n",
    "                j['issue'] = 'no-issue'\n",
    "\n",
    "            # write article metadata to csv\n",
    "            # csvwriter.writerow([idx] + [j['title']] + [] + [j['pub']] + [] + [j['length']] + [j['pub_date']])\n",
    "            csvwriter.writerow(['json/' + filename] + [j['title']] + [j['author']] + [j['pub']] + [j['volume']] + [j['issue']] + [j['pub_date']] + [j['length']])\n",
    "\n",
    "            # name article body file\n",
    "            padded_id = str(idx).zfill(len(str(len(sorted_json))))\n",
    "            \n",
    "            # write article body file to txt\n",
    "            with open(project_dir+'/' + text_files_clean_dir + '/'+ padded_id + '_.txt', 'w') as outfile:\n",
    "                if 'content_scrubbed' in j:\n",
    "                    outfile.write(string_cleaner(j['content_scrubbed']))\n",
    "                else:\n",
    "                    outfile.write(string_cleaner(j['content']))\n",
    "\n",
    "            idx = idx+1\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## CHECK METADATA\n",
    "\n",
    "!echo CHECK METADATA\n",
    "!echo\n",
    "!echo {metadata_dir} :\n",
    "!ls -1 {metadata_dir}\n",
    "!echo\n",
    "!echo {metadata_file_reorder} :\n",
    "!head -n 5 {metadata_file_reorder}\n",
    "!echo\n",
    "!echo CHECK TEXT FILES\n",
    "!echo\n",
    "!echo {text_files_clean_dir} :\n",
    "!ls -1 {text_files_clean_dir} | head\n",
    "!echo ...\n",
    "!ls -1 {text_files_clean_dir} | tail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL: build mallet topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## 1. run mallet -- import\n",
    "\n",
    "## build the mallet import command string\n",
    "mallet_import_args = '--input ' + project_dir + '/' + text_files_clean_dir + '/ ' \\\n",
    "  + '--output ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--keep-sequence ' \\\n",
    "  + '--remove-stopwords ' \\\n",
    "  + '--extra-stopwords ' + project_dir + '/' + stopwords_dir + '/' + stopwords_file + ' '\n",
    "mallet_import_command = 'mallet import-dir ' + mallet_import_args\n",
    "print(mallet_import_command+'\\n')\n",
    "\n",
    "## run mallet; capture and display output\n",
    "mout = !mallet import-dir {mallet_import_args}\n",
    "print('\\n'.join(mout)+'\\n')\n",
    "\n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel import done.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## 2. run mallet -- train\n",
    "\n",
    "## only generate diagnostics if feature available -- running on port 10000\n",
    "if(port==10000):\n",
    "    generate_diagnostics = True\n",
    "else:\n",
    "    generate_diagnostics = False\n",
    "    \n",
    "## build the mallet training command string\n",
    "mallet_train_args = '--input ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--num-topics ' + model_num_topics + ' ' \\\n",
    "  + '--optimize-interval 10 ' \\\n",
    "  + '--output-state ' + project_dir + '/' + model_dir + '/' + model_state + ' ' \\\n",
    "  + '--output-topic-keys ' + project_dir + '/' + model_dir + '/' + model_keys + ' ' \\\n",
    "  + '--output-doc-topics ' + project_dir + '/' + model_dir + '/' + model_composition + ' ' \\\n",
    "  + '--word-topic-counts-file ' + project_dir + '/' + model_dir + '/' + model_counts\n",
    "if use_random_seed == True:\n",
    "  mallet_train_args += ' --random-seed ' + model_random_seed\n",
    "if generate_diagnostics == True:\n",
    "  mallet_train_args += ' --diagnostics-file ' + project_dir + '/' + model_dir + '/diagnostics.xml'\n",
    "    \n",
    "mallet_train_command = 'mallet train-topics ' + mallet_train_args\n",
    "print(mallet_train_command+'\\n')\n",
    "\n",
    "print('\\nRunning:\\n')\n",
    "\n",
    "## run mallet\n",
    "!mallet train-topics {mallet_train_args}\n",
    "    \n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel training done.')\n",
    "\n",
    "if generate_diagnostics == True:\n",
    "    print('A diagnostics web page will be generated soon. This feature is not yet active. In the meantime, you can view the diagnostics.xml file in your model directory.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(generate_diagnostics):\n",
    "    print('View diagnostics.xml in Edit mode:')\n",
    "    diagnostics_edit_view = url_10000.replace('/notebooks/', '/edit/') + '/caches/model/diagnostics.xml'\n",
    "    from IPython.display import display, HTML\n",
    "    browser_link_html = HTML('<p><a href=\"' + diagnostics_edit_view + '\" target=\"_blank\"><strong>diagnostics.xml</strong></a></p>')\n",
    "    display(browser_link_html)\n",
    "else:\n",
    "    print('No diagnostics generated when run on 9999.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NEXT\n",
    "## Generate a link to the next notebook in the workflow\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "browser_link_html = HTML('<p>The topic model is built.</p><h2>Make a <a href=\"' + url_9999 + '/4_make_topic_browser.ipynb\" target=\"_blank\"><strong>dfrbrowser</strong> topic browser</a> (on 9999)</h2><p>...or...</p><h2>Make a <a href=\"' + url_10000 + '/6_browser_pyldavis.ipynb\" target=\"_blank\"><strong>pyLDAvis</strong> topic browser</a> (on 10000)</h2>')\n",
    "display(browser_link_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
