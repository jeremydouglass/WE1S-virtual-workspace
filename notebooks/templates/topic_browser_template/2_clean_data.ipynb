{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "Scrub text and remove stopwords.  \n",
    "Generates a new text_files_clean.\n",
    "\n",
    "-  v0.1 2016-10-19 split from new_dfr_project v.05 2016-10-19\n",
    "-  v0.2 2016-10-24 added metadata_dfrb.ipynb\n",
    "-  v0.3 2016-10-26 revised corpus_compare.py and added duplicate deleter step\n",
    "-  v0.4 2017-10-30 switch to Python 3\n",
    "-  v0.5 2017-10-31 merge metadata_dfrb.ipynb into notebook\n",
    "-  v0.6 2017-10-31 relocate all caches\n",
    "-  v0.7 2017-10-31 global settings file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INFO\n",
    "\n",
    "__author__    = 'Jeremy Douglass'\n",
    "__copyright__ = 'copyright 2016, The WE1S Project'\n",
    "__license__   = 'GPL'\n",
    "__version__   = '0.6'\n",
    "__email__     = 'jeremydouglass@gmail.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "## project directory\n",
    "project_dir    = %pwd\n",
    "print(project_dir)\n",
    "\n",
    "## import global project settings from config.py\n",
    "from settings import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCRUB TEXT\n",
    "\n",
    "# scrubs files from caches/text_files/\n",
    "# and puts output in caches/text_files_clean/\n",
    "# ... as per setting variables in scripts/scrub/config.py\n",
    "\n",
    "!mkdir -p {scrub_dir}\n",
    "!mkdir -p {text_files_clean_dir}\n",
    "\n",
    "%cd  $scrub_dir\n",
    "%run $scrub\n",
    "%cd  $project_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------\n",
    "## DOCKERFILE ADD\n",
    "## --------------\n",
    "# ImportError: No module named ftfy\n",
    "# Fix: add to Dockerfile:\n",
    "#   pip install ftfy\n",
    "#   pip2 install ftfy\n",
    "#   pip3 install ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE-DUPLICATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "## For help on script options:\n",
    "## %run scripts/deduplicate/corpus_compare.py -h \n",
    "\n",
    "## delete previous results\n",
    "!rm -f {dedup_dir}/{dedup_output}.csv\n",
    "!rm -f {dedup_dir}/{dedup_output}.log\n",
    "!rm -f {dedup_output}.log\n",
    "\n",
    "!mkdir -p {text_files_clean_dir}\n",
    "%run {dedup_dir}/{dedup} -i {text_files_clean_dir}/ -o {dedup_dir}/{dedup_name}.csv -l {dedup_dir}/{dedup_name}.log\n",
    "\n",
    "## --------------\n",
    "## FOR DockerFile\n",
    "## --------------\n",
    "## relies on sklearn\n",
    "## need to pip install or pip2 install or conda install scikit-learn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MINOR BUG\n",
    "## ---\n",
    "## corpus_compare.py is printing output multiple times if it is re-run in the notebook.\n",
    "## This has to do with setting the parser root handler -- do it, and warnings replicate, don't and no notebook output.\n",
    "## Output appears as an error message in a notebook and interrupts batch flow.\n",
    "## This also interrupts %run in a group notebook.\n",
    "##\n",
    "## ...fixed the double-notebook output by setting root logger to info and `logger.propagate = 0`\n",
    "## Tried removing all logger handlers at end of script to no effect.\n",
    "## However, .log text file is still written to twice, and notebook output is still pink/warning.\n",
    "## Tried moving all logger console calls from .debug to .info -- notebook output still pink.\n",
    "\n",
    "## in the future, could also try:\n",
    "## %capture [--no-stderr] [--no-stdout] [--no-display] [output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DELETE DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MERGE METADATA\n",
    "import os\n",
    "import csv\n",
    "with open(project_dir + '/' + dedup_dir + '/' + dedup_name + '.csv','r') as fin:\n",
    "    cfin = csv.reader(fin)\n",
    "    # print(cfin, None)\n",
    "    next(cfin) # skip header\n",
    "    for row in cfin:\n",
    "        if os.path.isfile(row[5]):\n",
    "            print('Deleting: ' + row[5])\n",
    "            os.remove(row[5])\n",
    "        else:\n",
    "            print('Missing:  '+ row[5])\n",
    "\n",
    "print('\\n-----\\nDuplicates deleted from:', dedup_dir + '/' + dedup_name + '.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RE-ORDER METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_in  = metadata_file\n",
    "csv_out = metadata_file_reorder\n",
    "\n",
    "## re-order column names\n",
    "\n",
    "## infieldnames provides names for the original column order\n",
    "infieldnames = 'id', 'journaltitle', 'pubdate', 'title', 'pagerange', 'author', 'volume', 'issue'\n",
    "## outfieldnames re-orders that name list into a new column order\n",
    "outfieldnames = 'id', 'title', 'author', 'journaltitle', 'volume', 'issue', 'pubdate', 'pagerange'\n",
    "\n",
    "## create reordered metadata file\n",
    "\n",
    "with open(csv_in, 'r') as infile, open(csv_out, 'a') as outfile:\n",
    "    ## input dict needs a list for column renaming\n",
    "    reader = csv.DictReader(infile, fieldnames=infieldnames)\n",
    "    ## skip outdated header row\n",
    "    next(reader, None)\n",
    "\n",
    "    ## output dict needs a reordered list for new column ordering\n",
    "    writer = csv.DictWriter(outfile, fieldnames=outfieldnames)\n",
    "    ## write automatic header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    ## write each row to new file with remapped column order\n",
    "    for row in reader:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print('\\n-----\\nReordered metadata to:', csv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEXT\n",
    "## Generate a link to the next notebook in the workflow\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "browser_link_html = HTML('<p>The data in ~/caches/text_files_clean/ is clean.</p><h2><a href=\"3_make_topic_model.ipynb\" target=\"_blank\">Next: Make the topic model.</h2>')\n",
    "display(browser_link_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
