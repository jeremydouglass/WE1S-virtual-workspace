{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TOPIC MODEL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will topic model project data imported in notebook 1 or in auxiliary notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the default number of topics to model is 50!\n",
    "\n",
    "The setting determining how many topics to model is located in the next cell (`model_num_topics = 50`). The default number of topics for every project is 50. If you wish to change this, you must change this number now. If you change your mind as you move down the notebook, you must re-run this top cell with the new number before importing and training your model for the new setting to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from settings import *\n",
    "model_num_topics = '50'\n",
    "jupyter_root = \"/home/jovyan\"\n",
    "project_dir = %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT FILES: Create plain-text files from json files for use in MALLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete old text files directory if exists and make new one\n",
    "!rm -fr {text_files_clean_dir}\n",
    "!mkdir -p {text_files_clean_dir}\n",
    "\n",
    "json_directory = 'caches/json/'\n",
    "\n",
    "sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "def string_cleaner(unistr):\n",
    "    \"\"\"Returns string in unaccented form, printable characters only.\"\"\"\n",
    "    unaccented = unidecode.unidecode(unistr)\n",
    "    printonly = ''.join(filter(lambda x:x in string.printable, unaccented))\n",
    "    return printonly\n",
    "\n",
    "idx=0\n",
    "for filename in sorted_json:\n",
    "    with open(os.path.join(json_directory, filename)) as f:\n",
    "            j = json.loads(f.read())\n",
    "    # name article body file\n",
    "    padded_id = str(idx).zfill(len(str(len(sorted_json))))\n",
    "    # write article body file to txt\n",
    "    with open(project_dir+'/' + text_files_clean_dir + '/'+ padded_id + '_.txt', 'w') as outfile:\n",
    "        if 'content' in j:\n",
    "            outfile.write(string_cleaner(j['content']))\n",
    "        else:\n",
    "            outfile.write(string_cleaner(j['content-unscrubbed']))\n",
    "\n",
    "    idx = idx+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check text files before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo CHECK TEXT FILES\n",
    "!echo\n",
    "!echo {text_files_clean_dir} :\n",
    "!ls -1 {text_files_clean_dir} | head\n",
    "!echo ...\n",
    "!ls -1 {text_files_clean_dir} | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL: build MALLET topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -r {model_dir}\n",
    "!mkdir -p {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output if running cell for the first time:\n",
    "`rm: cannot remove 'caches/model': No such file or directory`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run mallet -- import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## build the mallet import command string\n",
    "mallet_import_args = '--input ' + project_dir + '/' + text_files_clean_dir + '/ ' \\\n",
    "  + '--output ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--keep-sequence ' \\\n",
    "  + '--remove-stopwords ' \\\n",
    "  + '--extra-stopwords ' + project_dir + '/' + stopwords_dir + '/' + stopwords_file + ' '\n",
    "mallet_import_command = 'mallet import-dir ' + mallet_import_args\n",
    "print(mallet_import_command+'\\n')\n",
    "\n",
    "## run mallet; capture and display output\n",
    "mout = !mallet import-dir {mallet_import_args}\n",
    "print('\\n'.join(mout)+'\\n')\n",
    "\n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel import done.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run mallet -- train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "## build the mallet training command string\n",
    "mallet_train_args = '--input ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--num-topics ' + model_num_topics + ' ' \\\n",
    "  + '--optimize-interval 10 ' \\\n",
    "  + '--output-state ' + project_dir + '/' + model_dir + '/' + model_state + ' ' \\\n",
    "  + '--output-topic-keys ' + project_dir + '/' + model_dir + '/' + model_keys + ' ' \\\n",
    "  + '--output-doc-topics ' + project_dir + '/' + model_dir + '/' + model_composition + ' ' \\\n",
    "  + '--word-topic-counts-file ' + project_dir + '/' + model_dir + '/' + model_counts + ' ' \\\n",
    "  + '--diagnostics-file ' + project_dir + '/' + model_dir + '/diagnostics.xml'\n",
    "if use_random_seed == True:\n",
    "  mallet_train_args += ' --random-seed ' + model_random_seed\n",
    "# if generate_diagnostics == True:\n",
    " # mallet_train_args += ' --diagnostics-file ' + project_dir + '/' + model_dir + '/diagnostics.xml'\n",
    "    \n",
    "mallet_train_command = 'mallet train-topics ' + mallet_train_args\n",
    "print(mallet_train_command+'\\n')\n",
    "\n",
    "print('\\nRunning:\\n')\n",
    "\n",
    "## run mallet\n",
    "!mallet train-topics {mallet_train_args}\n",
    "    \n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel training done.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print link to diagnostics file. Diagnostics file is in `caches/model` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_project_dir = project_dir.replace('/home/jovyan/', '')\n",
    "diagnostics_edit_view = 'http://harbor.english.ucsb.edu:10000/edit/' + write_project_dir + '/caches/model/diagnostics.xml'\n",
    "\n",
    "print('View diagnostics.xml in Edit mode:')\n",
    "from IPython.display import display, HTML\n",
    "browser_link_html = HTML('<p><a href=\"' + diagnostics_edit_view + '\" target=\"_blank\"><strong>diagnostics.xml</strong></a></p>')\n",
    "display(browser_link_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCALING: Generate topic scaling metadata needed for dfr-browser and topic bubbles visualizations\n",
    "\n",
    "If you are running this cell for the first time in this project, you will get some warnings (output will be printed in red). This is ok; the scaling generation should still work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scale_topics.py.\n",
    "\n",
    "Create a topic_scaled.csv file from the Mallet state file.\n",
    "\n",
    "Combines code by Jeri E. Wieringa (https://github.com/jerielizabeth/Gospel-of-Health-Notebooks/blob/master/blogPosts/pyLDAvis_and_Mallet.ipynb)\n",
    "to transform Mallet data for use with pyLDAvis and uses code derived\n",
    "from pyLDAvis to calculate topic coordinates using MDS.\n",
    "\n",
    "Configure the paths to the mallet state and topic_scaled files below.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=E1101\n",
    "# pylint: disable=W1201\n",
    "\n",
    "\n",
    "# Python imports\n",
    "import gzip\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "# Set fallback for MDS scaling\n",
    "try:\n",
    "    from sklearn.manifold import MDS, TSNE\n",
    "    sklearn_present = True\n",
    "except ImportError:\n",
    "    sklearn_present = False\n",
    "from past.builtins import basestring\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "# Configure the input and output file paths\n",
    "output_state_file = os.path.join(project_dir + '/caches/model', 'topic-state.gz')\n",
    "topic_scaled_file = os.path.join(project_dir + '/caches/model', 'topic_scaled.csv')\n",
    "\n",
    "\n",
    "def __num_dist_rows__(array, ndigits=2):\n",
    "    return array.shape[0] - int((pd.DataFrame(array).sum(axis=1) < 0.999).sum())\n",
    "\n",
    "\n",
    "class ValidationError(ValueError):\n",
    "    \"\"\"Handle validation errors.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def _input_check(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):\n",
    "    ttds = topic_term_dists.shape\n",
    "    dtds = doc_topic_dists.shape\n",
    "    errors = []\n",
    "    def err(msg):\n",
    "        \"\"\"Append error message.\"\"\"\n",
    "        errors.append(msg)\n",
    "\n",
    "    if dtds[1] != ttds[0]:\n",
    "        err('Number of rows of topic_term_dists does not match number of columns of doc_topic_dists; both should be equal to the number of topics in the model.')\n",
    "\n",
    "    if len(doc_lengths) != dtds[0]:\n",
    "        err('Length of doc_lengths not equal to the number of rows in doc_topic_dists; both should be equal to the number of documents in the data.')\n",
    "\n",
    "    W = len(vocab)\n",
    "    if ttds[1] != W:\n",
    "        err('Number of terms in vocabulary does not match the number of columns of topic_term_dists (where each row of topic_term_dists is a probability distribution of terms for a given topic).')\n",
    "    if len(term_frequency) != W:\n",
    "        err('Length of term_frequency not equal to the number of terms in the vocabulary (len of vocab).')\n",
    "\n",
    "    if __num_dist_rows__(topic_term_dists) != ttds[0]:\n",
    "        err('Not all rows (distributions) in topic_term_dists sum to 1.')\n",
    "\n",
    "    if __num_dist_rows__(doc_topic_dists) != dtds[0]:\n",
    "        err('Not all rows (distributions) in doc_topic_dists sum to 1.')\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        return errors\n",
    "\n",
    "\n",
    "def _input_validate(*args):\n",
    "    res = _input_check(*args)\n",
    "    if res:\n",
    "        raise ValidationError('\\n' + '\\n'.join([' * ' + s for s in res]))\n",
    "\n",
    "\n",
    "def _jensen_shannon(_P, _Q):\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "\n",
    "def _pcoa(pair_dists, n_components=2):\n",
    "    \"\"\"Principal Coordinate Analysis.\n",
    "\n",
    "    AKA Classical Multidimensional Scaling\n",
    "    code referenced from skbio.stats.ordination.pcoa\n",
    "    https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py\n",
    "    \"\"\"\n",
    "    # pairwise distance matrix is assumed symmetric\n",
    "    pair_dists = np.asarray(pair_dists, np.float64)\n",
    "\n",
    "    # perform SVD on double centred distance matrix\n",
    "    n = pair_dists.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = - H.dot(pair_dists ** 2).dot(H) / 2\n",
    "    eigvals, eigvecs = np.linalg.eig(B)\n",
    "\n",
    "    # Take first n_components of eigenvalues and eigenvectors\n",
    "    # sorted in decreasing order\n",
    "    ix = eigvals.argsort()[::-1][:n_components]\n",
    "    eigvals = eigvals[ix]\n",
    "    eigvecs = eigvecs[:, ix]\n",
    "\n",
    "    # replace any remaining negative eigenvalues and associated eigenvectors with zeroes\n",
    "    # at least 1 eigenvalue must be zero\n",
    "    eigvals[np.isclose(eigvals, 0)] = 0\n",
    "    if np.any(eigvals < 0):\n",
    "        ix_neg = eigvals < 0\n",
    "        eigvals[ix_neg] = np.zeros(eigvals[ix_neg].shape)\n",
    "        eigvecs[:, ix_neg] = np.zeros(eigvecs[:, ix_neg].shape)\n",
    "\n",
    "    return np.sqrt(eigvals) * eigvecs\n",
    "\n",
    "\n",
    "def js_PCoA(distributions):\n",
    "    \"\"\"Perform dimension reduction.\n",
    "\n",
    "    Works via Jensen-Shannon Divergence & Principal Coordinate Analysis\n",
    "    (aka Classical Multidimensional Scaling)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcoa : array, shape (`n_dists`, 2)\n",
    "\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    return _pcoa(dist_matrix)\n",
    "\n",
    "\n",
    "def js_MMDS(distributions, **kwargs):\n",
    "    \"\"\"Perform dimension reduction.\n",
    "\n",
    "    Works via Jensen-Shannon Divergence & Metric Multidimensional Scaling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.MDS()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mmds : array, shape (`n_dists`, 2)\n",
    "\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = MDS(n_components=2, random_state=0, dissimilarity='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def js_TSNE(distributions, **kwargs):\n",
    "    \"\"\"Perform dimension reduction.\n",
    "\n",
    "    Works via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.TSNE()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tsne : array, shape (`n_dists`, 2)\n",
    "\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = TSNE(n_components=2, random_state=0, metric='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def _df_with_names(data, index_name, columns_name):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def _series_with_name(data, name):\n",
    "    if isinstance(data, pd.Series):\n",
    "        data.name = name\n",
    "        # ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)\n",
    "\n",
    "\n",
    "def _topic_coordinates(mds, topic_term_dists, topic_proportion):\n",
    "    K = topic_term_dists.shape[0]\n",
    "    mds_res = mds(topic_term_dists)\n",
    "    assert mds_res.shape == (K, 2)\n",
    "    mds_df = pd.DataFrame({'x': mds_res[:, 0], 'y': mds_res[:, 1], 'topics': range(1, K + 1), \\\n",
    "                            'cluster': 1, 'Freq': topic_proportion * 100})\n",
    "    # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26\n",
    "    return mds_df\n",
    "\n",
    "\n",
    "def get_topic_coordinates(topic_term_dists, doc_topic_dists, doc_lengths, \\\n",
    "            vocab, term_frequency, mds=js_PCoA, sort_topics=True):\n",
    "    \"\"\"Transform the topic model distributions and related corpus.\n",
    "\n",
    "    Creates the data structures needed for topic bubbles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_term_dists : array-like, shape (`n_topics`, `n_terms`)\n",
    "        Matrix of topic-term probabilities. Where `n_terms`\n",
    "        is `len(vocab)`.\n",
    "    doc_topic_dists : array-like, shape (`n_docs`, `n_topics`)\n",
    "        Matrix of document-topic probabilities.\n",
    "    doc_lengths : array-like, shape `n_docs`\n",
    "        The length of each document, i.e. the number of words\n",
    "        in each document. The order of the numbers should be\n",
    "        consistent with the ordering of the docs in `doc_topic_dists`.\n",
    "    vocab : array-like, shape `n_terms`\n",
    "        List of all the words in the corpus used to train the model.\n",
    "    term_frequency : array-like, shape `n_terms`\n",
    "        The count of each particular term over the entire corpus.\n",
    "        The ordering of these counts should correspond with\n",
    "        `vocab` and `topic_term_dists`.\n",
    "    mds : function or a string representation of function\n",
    "        A function that takes `topic_term_dists` as an input and\n",
    "        outputs a `n_topics` by `2`  distance matrix. The output\n",
    "        approximates the distance between topics. See :func:`js_PCoA`\n",
    "        for details on the default function. A string representation\n",
    "        currently accepts `pcoa` (or upper case variant), `mmds`\n",
    "        (or upper case variant) and `tsne` (or upper case variant),\n",
    "        if `sklearn` package is installed for the latter two.\n",
    "    sort_topics : sort topics by topic proportion (percentage of\n",
    "        tokens covered). Set to False to to keep original topic order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    topic_coordinates : A pandas dataframe containing\n",
    "        scaled x and y coordinates.\n",
    "\n",
    "    \"\"\"\n",
    "    # parse mds\n",
    "    if isinstance(mds, basestring):\n",
    "        mds = mds.lower()\n",
    "        if mds == 'pcoa':\n",
    "            mds = js_PCoA\n",
    "        elif mds in ('mmds', 'tsne'):\n",
    "            if sklearn_present:\n",
    "                mds_opts = {'mmds': js_MMDS, 'tsne': js_TSNE}\n",
    "                mds = mds_opts[mds]\n",
    "            else:\n",
    "                logging.warning('sklearn not present, switch to PCoA')\n",
    "                mds = js_PCoA\n",
    "        else:\n",
    "            logging.warning('Unknown mds `%s`, switch to PCoA' % mds)\n",
    "            mds = js_PCoA\n",
    "\n",
    "    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n",
    "    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n",
    "    term_frequency = _series_with_name(term_frequency, 'term_frequency')\n",
    "    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n",
    "    vocab = _series_with_name(vocab, 'vocab')\n",
    "    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n",
    "\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    if sort_topics:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    else:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum())\n",
    "\n",
    "    topic_order = topic_proportion.index\n",
    "    topic_term_dists = topic_term_dists.iloc[topic_order]\n",
    "\n",
    "    scaled_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)\n",
    "\n",
    "    return scaled_coordinates\n",
    "\n",
    "\n",
    "def extract_params(statefile):\n",
    "    \"\"\"Extract the alpha and beta values from the statefile.\n",
    "\n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET.\n",
    "    Returns:\n",
    "        tuple: alpha (list), beta\n",
    "\n",
    "    \"\"\"\n",
    "    with gzip.open(statefile, 'r') as state:\n",
    "        params = [x.decode('utf8').strip() for x in state.readlines()[1:3]]\n",
    "    return (list(params[0].split(\":\")[1].split(\" \")), float(params[1].split(\":\")[1]))\n",
    "\n",
    "\n",
    "def state_to_df(statefile):\n",
    "    \"\"\"Transform state file into pandas dataframe.\n",
    "\n",
    "    The MALLET statefile is tab-separated, and the first two rows contain the alpha and beta hypterparamters.\n",
    "\n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET.\n",
    "    Returns:\n",
    "        datframe: topic assignment for each token in each document of the model\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.read_csv(statefile,\n",
    "                        compression='gzip',\n",
    "                        sep=' ',\n",
    "                        skiprows=[1, 2]\n",
    "                        )\n",
    "\n",
    "\n",
    "def pivot_and_smooth(df, smooth_value, rows_variable, cols_variable, values_variable):\n",
    "    \"\"\"Turn the pandas dataframe into a data matrix.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): aggregated dataframe\n",
    "        smooth_value (float): value to add to the matrix to account for the priors\n",
    "        rows_variable (str): name of dataframe column to use as the rows in the matrix\n",
    "        cols_variable (str): name of dataframe column to use as the columns in the matrix\n",
    "        values_variable(str): name of the dataframe column to use as the values in the matrix\n",
    "    Returns:\n",
    "        dataframe: pandas matrix that has been normalized on the rows.\n",
    "\n",
    "    \"\"\"\n",
    "    matrix = df.pivot(index=rows_variable, columns=cols_variable, values=values_variable).fillna(value=0)\n",
    "    matrix = matrix.values + smooth_value\n",
    "\n",
    "    normed = sklearn.preprocessing.normalize(matrix, norm='l1', axis=1)\n",
    "\n",
    "    return pd.DataFrame(normed)\n",
    "\n",
    "\n",
    "def convert_mallet_data(state_file):\n",
    "    \"\"\"Convert Mallet data to a structure compatible with pyLDAvis.\n",
    "\n",
    "    Args:\n",
    "        output_state_file (string): Mallet state file\n",
    "\n",
    "    Returns:\n",
    "        data: dict containing pandas dataframes for the pyLDAvis prepare method.\n",
    "\n",
    "    \"\"\"\n",
    "    params = extract_params(state_file)\n",
    "    alpha = [float(x) for x in params[0][1:]]\n",
    "    beta = params[1]\n",
    "    df = state_to_df(state_file)\n",
    "    # Ensure that NaN is a string\n",
    "    df['type'] = df.type.astype(str)\n",
    "    # Get document lengths from statefile\n",
    "    docs = df.groupby('#doc')['type'].count().reset_index(name='doc_length')\n",
    "    # Get vocab and term frequencies from statefile\n",
    "    vocab = df['type'].value_counts().reset_index()\n",
    "    vocab.columns = ['type', 'term_freq']\n",
    "    vocab = vocab.sort_values(by='type', ascending=True)\n",
    "    phi_df = df.groupby(['topic', 'type'])['type'].count().reset_index(name='token_count')\n",
    "    phi_df = phi_df.sort_values(by='type', ascending=True)\n",
    "    phi = pivot_and_smooth(phi_df, beta, 'topic', 'type', 'token_count')\n",
    "    theta_df = df.groupby(['#doc', 'topic'])['topic'].count().reset_index(name='topic_count')\n",
    "    theta = pivot_and_smooth(theta_df, alpha, '#doc', 'topic', 'topic_count')\n",
    "    data = {'topic_term_dists': phi,\n",
    "            'doc_topic_dists': theta,\n",
    "            'doc_lengths': list(docs['doc_length']),\n",
    "            'vocab': list(vocab['type']),\n",
    "            'term_frequency': list(vocab['term_freq'])\n",
    "        }\n",
    "    return data\n",
    "\n",
    "# Convert the Mallet data and export the topic_scale.csv file\n",
    "converted_data = convert_mallet_data(output_state_file)\n",
    "topic_coordinates = get_topic_coordinates(**converted_data)\n",
    "topic_coordinates.to_csv(topic_scaled_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT NOTEBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "write_project_dir = project_dir.replace('/home/jovyan/', '')\n",
    "next_link = 'http://harbor.english.ucsb.edu:10000/notebooks/' + write_project_dir\n",
    "\n",
    "next_link_html = HTML('<h2>Next:</h2><p><ul><li>Go to <a href=\"' + next_link + '/3_browser_dfrbrowser.ipynb\" target=\"_blank\"><strong>Notebook 3</a></strong> to make a dfrbrowser.</li><li>Go to <a href=\"' + next_link + '/5_browser_pyldavis.ipynb\" target=\"_blank\"><strong>Notebook 5</a></strong> to make a PyLDAVis visualization.</li><li>Go to <a href=\"' + next_link + '/6_browser_topic_bubbles.ipynb\" target=\"_blank\"><strong>Notebook 6</a></strong> to make a topic bubbles visualization.</li></ul></p>')\n",
    "display(next_link_html)\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
