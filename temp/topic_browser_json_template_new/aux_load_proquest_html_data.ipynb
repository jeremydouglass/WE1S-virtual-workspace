{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PROQUEST HTML DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook enables you to upload one or more HTML results files from searches performed in ProQuest. The files are converted into valid JSON manifests, allowing the rest of the Virtual Workspace to work with them.\n",
    "\n",
    "Run the **Settings** and **Basic Setup** cells _before_ uploading your ProQuest HTML files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser as dateparser\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "project_dir = %pwd\n",
    "proquest_dir = 'caches/proquest_sources'\n",
    "results_dir = 'caches/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIC SETUP\n",
    "\n",
    "This cell creates the required directory into which you will upload your ProQuest HTML file(s) and the directory into which the json files you create will be put. Please run this cell and then follow the instructions for uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r caches/proquest_sources # if this is the first time you have run this cell, this will create output that reads\n",
    "# rm: cannot remove 'caches/proquest_sources': No such file or directory\n",
    "# this is normal and expected\n",
    "!mkdir -p caches/proquest_sources\n",
    "!rm -r caches/json\n",
    "!mkdir -p caches/json\n",
    "\n",
    "msg = '''\n",
    "<h3>You are now ready to upload your material.</h3>\n",
    "<p>Navigate to the <code>caches/proquest_sources</code> folder in this project's directory and click the \"Upload\" button to upload each ProQuest HTML file you want to import.\n",
    "<p>Note that this data should be in .html format.</p>\n",
    "'''\n",
    "output = HTML(msg)\n",
    "\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILENAMES: Determine the filenames of the ProQuest HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_list = []\n",
    "for (dirname, _dirs, files) in os.walk(proquest_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.html'):\n",
    "            filepath = os.path.join(dirname.split(proquest_dir)[1], filename)\n",
    "            datafile_list.append(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE JSON: Create the JSON files\n",
    "\n",
    "This works by chopping up single html file into separate chunks according to a set delimiter. Then, we use a monstrous for loop to strip info from each html file using combo of Beautiful soup and regex. We put this data into a dictionary. This is messy because ProQuest html files are very poorly tagged. We then dump this dictionary into json files in the project's json directory. We save each json file with a padded id as the filename in a subdirectory determined by the original ProQuest file it came from (so, all individual articles from the same original ProQuest results html file will be in the same subdirectory). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This will throw a lot of errors right now. That doesn't matter. Once the for loop completes you can run the next cell.\n",
    "\n",
    "We will be refactoring the script to eliminate this excessive error reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_split = []\n",
    "article = {}\n",
    "for file in datafile_list:\n",
    "    file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    sub_dir = results_dir + file_name\n",
    "    if not os.path.isdir(sub_dir):\n",
    "        os.makedirs(sub_dir)\n",
    "    filepath_html = proquest_dir + '/' + file\n",
    "    with open(filepath_html, 'rt') as infile:\n",
    "       html_file = infile.read()\n",
    "    html_split = html_file.split('<div style=\"margin-bottom:20px;border-bottom:2px solid #ccc;padding-bottom:5px\">')\n",
    "    idx = 0\n",
    "    for html in html_split:\n",
    "        padded_id = str(idx).zfill(len(str(len(html_split))))\n",
    "        article_path = proquest_dir + html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        p_tags = soup.find_all('p')\n",
    "        everything = str(soup)\n",
    "        body_tags = soup.find_all('body')\n",
    "        # article title\n",
    "        try:\n",
    "            just_second = p_tags[1]\n",
    "            article_title = just_second.get_text()\n",
    "            article['title'] = article_title\n",
    "        except Exception as exc:\n",
    "            print('! error finding article_title')\n",
    "            article['title'] = ''\n",
    "        # doc-id\n",
    "        try:\n",
    "            doc_id = str(soup.find('a'))\n",
    "            doc_id = re.sub('<a name=\"', '', doc_id)\n",
    "            doc_id = re.sub('\"></a>', '', doc_id)\n",
    "            article['doc_id'] = doc_id\n",
    "        except Exception as exc:\n",
    "            print('! error finding doc-id')\n",
    "            article['doc_id'] = ''\n",
    "        # pub\n",
    "        try:\n",
    "            pub_title_obj = re.search('<strong>Publication title: </strong>(.+?)</p>', everything)\n",
    "            pub_title = pub_title_obj.group(1)\n",
    "            article['pub'] = pub_title\n",
    "        except Exception as exc:\n",
    "            print('! error finding pub_title')\n",
    "            article['pub'] = ''\n",
    "        # pub-date\n",
    "        bad_date = '1900-01-01T00:00:00Z'\n",
    "        try:\n",
    "            pub_date_obj = re.search('<strong>Publication date: </strong>(.+?)</p>', everything)\n",
    "            if pub_date_obj:\n",
    "                pub_date = pub_date_obj.group(1)\n",
    "            pub_date = re.sub('([a-zA-Z]*)\\/([a-zA-Z]*)', r'\\1', pub_date)\n",
    "            date=''\n",
    "            parse_date = pub_date\n",
    "            while not date:\n",
    "                try:\n",
    "                    date = dateparser.parse(parse_date)\n",
    "                except Exception as exc:\n",
    "                    print(' ! error parsing pub_date', exc)\n",
    "                    parse_date = ' '.join(parse_date.split(' ')[:-1])\n",
    "                if not parse_date or parse_date.isspace():\n",
    "                    break\n",
    "            date_out = date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        except Exception as exc:\n",
    "            print('! error finding pub-date')\n",
    "            date_out = bad_date\n",
    "        # volume\n",
    "        try:\n",
    "            volume_obj = re.search('<strong>Volume: </strong>(.+?)</p>', everything)\n",
    "            volume = volume_obj.group(1)\n",
    "            article['volume'] = volume\n",
    "        except Exception as exc:\n",
    "            print('! error finding volume')\n",
    "            article['volume'] = ''\n",
    "        # issue\n",
    "        try:\n",
    "            issue_obj = re.search('<strong>Issue: </strong>(.+?)</p>', everything)\n",
    "            issue = issue_obj.group(1)\n",
    "            article['issue'] = issue\n",
    "        except Exception as exc:\n",
    "            print('! error finding issue')\n",
    "            article['issue'] = ''\n",
    "        # content\n",
    "        try:\n",
    "            just_second_body = body_tags[1]\n",
    "            article_text = just_second_body.get_text()\n",
    "            article['content'] = article_text\n",
    "        except Exception as exc:\n",
    "            print('! error finding article content')\n",
    "            article['content'] = ''\n",
    "        # length\n",
    "        try:\n",
    "            length = len(article_text)\n",
    "            article['length'] = length\n",
    "        except Exception as exc:\n",
    "            print('! error finding length')\n",
    "        # section\n",
    "        try:\n",
    "            section_obj = re.search('<strong>Section: </strong>(.+?)</p>', everything)\n",
    "            section = section_obj.group(1)\n",
    "            article['section'] = section\n",
    "        except Exception as exc:\n",
    "            print('! error finding section')\n",
    "            article['section'] = ''\n",
    "        # author\n",
    "        try:\n",
    "            just_author = p_tags[2]\n",
    "            author = just_author.get_text()\n",
    "            author = re.sub('Author: ', '', author)\n",
    "            article['author'] = author\n",
    "        except Exception as exc:\n",
    "            print('! error finding author')\n",
    "            article['author'] = ''\n",
    "        # copyright\n",
    "        try:\n",
    "            pub_info_obj = re.search('<strong>Publication info: </strong>(.+?)</p>', everything)\n",
    "            pub_info = pub_info_obj.group(1)\n",
    "            article['copyright'] = pub_info\n",
    "        except Exception as exc:\n",
    "            print('! error finding copyright')\n",
    "            article['copyright'] = ''\n",
    "        # database\n",
    "        try:\n",
    "            database_obj = re.search('<strong>Database: </strong>(.+?)</p>', everything)\n",
    "            database = database_obj.group(1)\n",
    "            article['database'] = database\n",
    "        except Exception as exc:\n",
    "            print('! error finding database')\n",
    "            article['database'] = 'ProQuest'\n",
    "        #name\n",
    "        try:\n",
    "            pub_title_transform = ''.join(pub_title.split())\n",
    "            pub_title_transform = re.sub(r'[^\\w]','',pub_title_transform)\n",
    "            name = pub_title_transform + '_' + file_name\n",
    "        except Exception as exc:\n",
    "            print('! error adding name')\n",
    "            article['name'] = ''\n",
    "        try:\n",
    "            article['attachment-id'] = ''\n",
    "            article['name'] = name \n",
    "            article['namespace'] = \"we1sv2.0\"\n",
    "            article['metapath'] = \"Corpus,\" + name + \",RawData\"\n",
    "            article['pub_date'] = date_out\n",
    "            article['raw_date'] = pub_date\n",
    "        except Exception as exc:\n",
    "            print('! error adding extra keys')\n",
    "        with open(sub_dir + '/' + file_name + '_' + padded_id + '_.json', 'w') as outfile:\n",
    "            json.dump(article, outfile, sort_keys=True, indent=2)           \n",
    "        idx = idx+1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the first json file in each subdirectory you just created, since it is a list of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(results_dir):\n",
    "    for name in files:\n",
    "        if '_00_.json' in name:\n",
    "            first_file = os.path.join(path, name)\n",
    "            os.remove(first_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When WMS and databases are set up: Zip up each subdirectory and delete unzipped directory, send zips of jsons to mongodb. Don't run the below cell for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path, subdirs, files in os.walk(results_dir, topdown=True):\n",
    "    # for subdir in subdirs:\n",
    "     #  single_subdir = os.path.join(path, subdir)\n",
    "     #  shutil.make_archive(single_subdir,'zip',single_subdir)\n",
    "     #  shutil.rmtree(single_subdir)\n",
    "\n",
    "# then send zips to mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten subdirectory structure to create one `caches/json` directory with all json files in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(results_dir):\n",
    "    for subdir in subdirs:\n",
    "        single_subdir = os.path.join(path, subdir)\n",
    "        !cp {single_subdir}/* {results_dir}\n",
    "        !rm -r {single_subdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT NOTEBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "write_project_dir = project_dir.replace('/home/jovyan/', '')\n",
    "next_link = 'http://harbor.english.ucsb.edu:10000/notebooks/' + write_project_dir\n",
    "\n",
    "next_link_html = HTML('<h2>Next:</h2><p><ul><li>Go to <a href=\"' + next_link + '/1_import_data.ipynb\" target=\"_blank\"><strong>Notebook 1</a></strong> if you want to further filter, randomly select from, scrub, or de-duplicate your data. If you use notebook 1, do not run the cells with the following titles: BROWSE, LIST, IMPORT. </li><li>If you do not want to filter or de-duplicate your data, go to <a href=\"' + next_link + '/2_topic_model_data.ipynb\" target=\"_blank\"><strong>Notebook 2</a></strong> to model your data.</li></ul></p>')\n",
    "display(next_link_html)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
