{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMPORT DATA\n",
    "\n",
    "### This notebook will import selected data for use in the project. You have the options to filter, randomly sample from, and de-duplicate data. Data must be in required format, meaning a zipped folder of json files with required we1s metadata fields. If you have data in plain-text form, or HTML files of ProQuest search results, begin with the appropriate `aux_` notebook for your data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# import global project settings from config.py\n",
    "from settings import *\n",
    "\n",
    "# set jupyter_root and project directory\n",
    "jupyter_root = \"/home/jovyan\"\n",
    "project_dir = %pwd\n",
    "print(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BROWSE: search zip filenames for keywords\n",
    "\n",
    "Choose `search_text` to filter available data files. If you are searching for a specific word or phrase, enter it WITHIN the single quotes below. Note that you will be searching the filenames of the data zip folders stored on harbor (usually in the `data` directory). If you want to simply list all of the files in a specific data directory, change the value of the `search_text` variable below to `None` WITHOUT single quotes (so the line should read `search_text=None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_text='search-text-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell and review the results. The default is to search through the `data/data-new/` directory. \n",
    "If your data is in a different location on harbor, change the `data_directory` variable to the directory you want to search, making sure to KEEP the slash at the end of the directory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_directory = 'data/data-new/'\n",
    "filespath = jupyter_root + '/' + data_directory\n",
    "print(\"datafile_list = [\")\n",
    "if search_text:\n",
    "    for (dirname, _dirs, files) in os.walk(filespath):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.zip') and search_text in filename:\n",
    "                filepath = os.path.join(dirname.split(filespath)[1], filename)\n",
    "                print(\"    '\" + filepath + \"',\")\n",
    "else:\n",
    "    for (dirname, _dirs, files) in os.walk(filespath):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.zip'):\n",
    "                filepath = os.path.join(dirname.split(filespath)[1], filename)\n",
    "                print(\"    '\" + filepath + \"',\")\n",
    "print(\"                 ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST: define which zips will be used to import JSON files\n",
    "\n",
    "To only import the zip files you found above, copy the entire cell output above and replace the datafile_list array in the following cell. Each filename should be surrounded by single quotes, and after each filename there should be a comma (for the last filename in the list it doesn't matter if you include the commor not). Then run the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile_list = ['164282_deseretmorningnewssaltlakecity_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1980-01-01_1980-12-31.zip',\n",
    "'164282_deseretmorningnews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'300814_theforward_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'438278_thefreepressfernie_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT: copy JSON from zip files to project cache\n",
    "\n",
    "JSON files will be stored in the `/caches/` project directory. Original zip source data remains untouched. Remember to define your `data_directory` again below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "data_directory = 'data/data-new'\n",
    "filespath = jupyter_root + '/' + data_directory\n",
    "\n",
    "!rm -r caches/json\n",
    "!mkdir -p caches/json\n",
    "\n",
    "for datafile in datafile_list:\n",
    "    datapath = filespath + '/' + datafile\n",
    "    !unzip -j -o -u \"{datapath}\" \"*.json\" -d caches/json > /dev/null\n",
    "\n",
    "!ls caches/json | wc -l\n",
    "    \n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTER: delete non-matching JSON\n",
    "\n",
    "If you want to filter out any articles that do not contain a required keyword or phrase -- e.g. 'humanities' -- then write word here, between the single quotes in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "required_phrase = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the filter to delete JSON files that do not match. If no filter is defined, this step will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os, re, json\n",
    "\n",
    "if required_phrase:\n",
    "    \n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    del_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            json_content = json_decoded['content']\n",
    "            if not re.search(required_phrase, json_content, re.IGNORECASE):\n",
    "                os.remove(os.path.join(json_directory, filename))\n",
    "                del_count += 1\n",
    "                if(del_count%10==0):\n",
    "                    print('. ', end='')\n",
    "    new_num_docs = len(os.listdir(json_directory))\n",
    "    print('Number of documents deleted: ' + str(del_count))\n",
    "    print('Number of documents containing \"' + required_phrase + '\": ' + str(new_num_docs))\n",
    "else:\n",
    "    print('No required phrase, no documents deleted.')\n",
    "\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM SAMPLE: Select x number of articles to analyze from imported files\n",
    "\n",
    "Use the following cells if you want to randomly sample the total number of json files imported (and/or filtered) so that you only select a sample for analysis. Set the `selection` variable to the number of articles you want to randomly sample. Skip the cells in this section if you want to include all of the imported articles in your analysis.\n",
    "\n",
    "If your data contains duplicates, and you choose to sample your data before detecting and deleting duplicates (see below), please note that some duplicates may end up in your random sample. You can choose to delete or to keep them in the cells under the \"DE-DUPLICATE\" heading, but if you choose to delete them, you may eliminate articles from your sample, causing the total number of articles in your sample to go down. To avoid this, you can choose to run the \"DE-DUPLICATE\" cells *before* running the cells under \"RANDOM SAMPLE,\" but this means you will be running your de-duplication detection against your entire imported dataset. If you have imported lots of articles, de-duplication can take awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "selection = 1200 #change this number to the number of articles you want to randomly sample. \n",
    "# Do not use commas in the number.\n",
    "\n",
    "json_list = os.listdir(\"caches/json\")\n",
    "sample = random.sample(json_list, selection)\n",
    "\n",
    "# preview sample\n",
    "print(sample[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the contents of random selection to `caches/json_sample` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfastcopy\n",
    "\n",
    "!rm -r caches/json_sample\n",
    "!mkdir -p caches/json_sample\n",
    "\n",
    "for item in sample:\n",
    "    filepath = \"caches/json/\" + item\n",
    "#     !mv '{filepath}' caches/json_sample\n",
    "    shutil.copy(filepath, 'caches/json_sample/')\n",
    "    \n",
    "!ls caches/json_sample | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move contents of `caches/json_sample` to new `caches/json` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r caches/json\n",
    "!mkdir caches/json\n",
    "!mv caches/json_sample/* caches/json\n",
    "!rm -r caches/json_sample\n",
    "!ls caches/json | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are dealing with data COLLECTED from LexisNexis AFTER 2.10.19, you do not need to run SCRUB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB: add scrubbed content to JSON\n",
    "\n",
    "Scrubbing is performed on each article JSON file, and the results are stored in a new key in the JSON file.\n",
    "\n",
    "-  To perform, set this step to True.\n",
    "-  If an article is already scrubbed it will be skipped unless rescrub is True.\n",
    "-  To reduce the JSON cache size, set delete original content. If original content is deleted then scrubbing cannot be repeated without re-exporting JSON from zip above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_scrub = True\n",
    "do_scrub_rescrub = False\n",
    "do_scrub_delete_original_content = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to scrub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "from scripts.scrub.scrub import scrub\n",
    "\n",
    "if do_scrub:\n",
    "\n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    scrub_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            if 'content' in json_decoded and (not 'content-unscrubbed' in json_decoded or do_scrub_rescrub):\n",
    "                json_decoded['content-unscrubbed'] = json_decoded['content']\n",
    "                json_decoded['content'] = scrub(json_decoded['content'])\n",
    "                scrub_changed = True\n",
    "#             if 'content' in json_decoded and (not 'content_scrubbed' in json_decoded or do_scrub_rescrub):\n",
    "#                 json_decoded['content_scrubbed'] = scrub(json_decoded['content'])\n",
    "#                 scrub_changed = True\n",
    "            if do_scrub_delete_original_content and 'content-unscrubbed' in json_decoded and 'content' in json_decoded:\n",
    "                json_decoded.pop('content-unscrubbed', None)\n",
    "                scrub_changed = True\n",
    "        if scrub_changed:\n",
    "            with open(fpath, 'w') as json_file:\n",
    "                json.dump(json_decoded, json_file)\n",
    "            scrub_count += 1\n",
    "            ## progress indicator\n",
    "            if(scrub_count%100==0):\n",
    "                print('. ', end='')\n",
    "    print('Scrubbed ' + str(scrub_count) + ' files.')\n",
    "else:\n",
    "    print('Skipping scrub.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "Run the following cells if you want to detect duplicates in imported data and delete them. Right now, de-duplication fails on collections of data greater than ~22,000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_dedupe = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "## For help on script options:\n",
    "## %run scripts/deduplicate/corpus_compare.py -h \n",
    "\n",
    "if do_dedupe:\n",
    "\n",
    "    print(project_dir)\n",
    "    print(dedup_dir)\n",
    "    print(dedup_name)\n",
    "    \n",
    "    ## delete previous results\n",
    "    !rm -f {dedup_dir}/{dedup_output}.csv\n",
    "    !rm -f {dedup_dir}/{dedup_output}.log\n",
    "    !rm -f {dedup_output}.log\n",
    "\n",
    "    %run {dedup_dir}/{dedup} -i caches/json/ -f *.json --threshold 0.8 -o {dedup_dir}/{dedup_name}.csv -l {dedup_dir}/{dedup_name}.log\n",
    "\n",
    "## --------------\n",
    "## FOR DockerFile\n",
    "## --------------\n",
    "## relies on sklearn\n",
    "## need to pip install or pip2 install or conda install scikit-learn?\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete detected duplicates. Do not run if you don't want to delete duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MERGE METADATA\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "if do_dedupe:\n",
    "    with open(project_dir + '/' + dedup_dir + '/' + dedup_name + '.csv','r') as fin:\n",
    "        cfin = csv.reader(fin)\n",
    "        # print(cfin, None)\n",
    "        next(cfin) # skip header\n",
    "        for row in cfin:\n",
    "            if os.path.isfile(row[5]):\n",
    "                print('Deleting: ' + row[5])\n",
    "                os.remove(row[5])\n",
    "            else:\n",
    "                print('Missing:  '+ row[5])\n",
    "    print('\\n-----\\nDuplicates deleted from:', dedup_dir + '/' + dedup_name + '.csv')\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_project_dir = project_dir.replace('/home/jovyan/', '')\n",
    "next_link = 'http://harbor.english.ucsb.edu:10000/notebooks/' + write_project_dir + '/2_topic_model_data.ipynb'\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "next_link_html = HTML('<h2>Next:</h2><p>Go to <a href=\"' + next_link + '\" target=\"_blank\"><strong>Notebook 2</a></strong> to model your data.</p>')\n",
    "display(next_link_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
