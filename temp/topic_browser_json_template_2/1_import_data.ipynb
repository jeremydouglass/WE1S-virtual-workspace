{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING NOTEBOOK\n",
    "## Run top cell, then click its \"RUN ALL\" button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<button id=\"do_run_all\" style=\"font-size:32px;\">RUN ALL</button>\n",
    "<script>\n",
    "$(\"#do_run_all\").click(\n",
    "    function () {\n",
    "        // assign port to Python variable\n",
    "        var port_command = \"port = \" + location.port + \"\";\n",
    "        Jupyter.notebook.kernel.execute(port_command);\n",
    "        // write notebook url bases for target ports\n",
    "        var url_parser = document.createElement(\"a\");\n",
    "        url_parser.href = location.href.substring(0, location.href.lastIndexOf(\"/\"));\n",
    "        url_parser.port = \"9999\";\n",
    "        var url_9999_command = \"url_9999 = '\" + url_parser.href + \"'\";\n",
    "        url_parser.port = \"10000\";\n",
    "        var url_10000_command = \"url_10000 = '\" + url_parser.href + \"'\";\n",
    "        // assign to Python variables\n",
    "        Jupyter.notebook.kernel.execute(url_9999_command);\n",
    "        Jupyter.notebook.kernel.execute(url_10000_command);\n",
    "        // in %%javascript cell only:\n",
    "        // element.html(port_command + '<br>' + url_9999_command + '<br>' + url_10000_command);\n",
    "        $(\"#run_all_cells\").click();\n",
    "    });\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## IMPORT\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "## SETTINGS\n",
    "\n",
    "## project directory\n",
    "project_dir = %pwd\n",
    "print(project_dir)\n",
    "\n",
    "## import global project settings from config.py\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect port / path / url environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// for manual running if not using run-all button\n",
    "\n",
    "// detect port\n",
    "var port = location.port;\n",
    "// assign to Python variable\n",
    "var port_command = \"port = \" + port + \"\";\n",
    "\n",
    "// write notebook url bases for target ports\n",
    "var url_parser = document.createElement(\"a\");\n",
    "url_parser.href = location.href.substring(0, location.href.lastIndexOf(\"/\"));\n",
    "url_parser.port = \"9999\";\n",
    "var url_9999_command = \"url_9999 = '\" + url_parser.href + \"'\";\n",
    "url_parser.port = \"10000\";\n",
    "var url_10000_command = \"url_10000 = '\" + url_parser.href + \"'\";\n",
    "\n",
    "// assign to Python variables\n",
    "Jupyter.notebook.kernel.execute(port_command);\n",
    "Jupyter.notebook.kernel.execute(url_9999_command);\n",
    "Jupyter.notebook.kernel.execute(url_10000_command);\n",
    "\n",
    "// display results\n",
    "element.html(port_command + '<br>' + url_9999_command + '<br>' + url_10000_command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(port)\n",
    "    print(url_9999)\n",
    "    print(url_10000)\n",
    "except NameError as e:\n",
    "    print(\"Not defined.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define non-url root path based on port\n",
    "jupter_root = \"/home/jovyan\"\n",
    "if (port==9999):\n",
    "    jupter_root = jupter_root + \"/work\"\n",
    "print('jupter_root =', jupter_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var url_parser = document.createElement(\"a\");\n",
    "url_parser.href = location.href;\n",
    "if(url_parser.port == \"10000\"){\n",
    "    url_parser.port = \"9999\";\n",
    "} else {\n",
    "    url_parser.port = \"10000\";\n",
    "}\n",
    "element.html('<p>If you wish, <strong>save first</strong> and then <a href=\"' + url_parser.href + '\"\"><strong>switch to this notebook on ' + url_parser.port + '</strong></a>.')\n",
    "\n",
    "// // assign to Python variable\n",
    "// IPython.notebook.kernel.execute(url_switch_command);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BROWSE: search zip filenames for keywords\n",
    "\n",
    "Choose search_text to filter available data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_text='mexico'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filespath = jupter_root + '/data/'\n",
    "print(\"datafile_list = [\")\n",
    "for (dirname, _dirs, files) in os.walk(filespath):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.zip') and search_text in filename:\n",
    "            filepath = os.path.join(dirname.split(filespath)[1], filename)\n",
    "            print(\"    '\" + filepath + \"',\")\n",
    "print(\"                 ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST: define which zips will be used to import JSON files\n",
    "\n",
    "Optionally cut-paste the entire cell output and replace the datafile_list array in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondatadir = jupter_root + '/data/data-new/'\n",
    "datafile_list = ['164282_deseretmorningnewssaltlakecity_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'6742_thenewyorktimes_bodypluralhumanitiesorhleadpluralhumanities_1980-01-01_1980-12-31.zip',\n",
    "'164282_deseretmorningnews_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'300814_theforward_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip',\n",
    "'438278_thefreepressfernie_bodypluralhumanitiesorhleadpluralhumanities_2017-01-01_2017-12-31.zip']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT: copy JSON from zip files to cache\n",
    "\n",
    "JSON files will be stored in the /caches/ project directory. Original zip source data remains untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "!rm -r caches/json\n",
    "!mkdir -p caches/json\n",
    "\n",
    "for datafile in datafile_list:\n",
    "    datapath = jsondatadir + datafile\n",
    "    !unzip -j -o -u \"{datapath}\" -d caches/json > /dev/null\n",
    "\n",
    "!ls caches/json | wc -l\n",
    "    \n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTER: delete non-matching JSON\n",
    "\n",
    "If you want to filter out any articles that do not contain a required keyword or phrase -- e.g. 'humanities' -- then write word here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "required_phrase = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the filter to delete JSON files that do not match. If no filter is defined, this step will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os, re, json\n",
    "\n",
    "if required_phrase:\n",
    "    \n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    del_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            json_content = json_decoded['content']\n",
    "            if not re.search(required_phrase, json_content, re.IGNORECASE):\n",
    "                os.remove(os.path.join(json_directory, filename))\n",
    "                del_count += 1\n",
    "                if(del_count%10==0):\n",
    "                    print('. ', end='')\n",
    "    new_num_docs = len(os.listdir(json_directory))\n",
    "    print('Number of documents deleted: ' + str(del_count))\n",
    "    print('Number of documents containing \"' + required_phrase + '\": ' + str(new_num_docs))\n",
    "else:\n",
    "    print('No required phrase, no documents deleted.')\n",
    "\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB: add scrubbed content to JSON\n",
    "\n",
    "Scrubbing is performed on each article JSON file, and the results are stored in a new key in the JSON file.\n",
    "\n",
    "-  To perform, set this step to True.\n",
    "-  If an article is already scrubbed it will be skipped unless rescrub is True.\n",
    "-  To reduce the JSON cache size, set delete original content. If original content is deleted then scrubbing cannot be repeated without re-exporting JSON from zip above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_scrub = True\n",
    "do_scrub_rescrub = False\n",
    "do_scrub_delete_original_content = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to scrub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "from scripts.scrub.scrub import scrub\n",
    "\n",
    "if do_scrub:\n",
    "\n",
    "    json_directory = 'caches/json/'\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "\n",
    "    scrub_count = 0\n",
    "    for filename in sorted_json:\n",
    "        fpath = os.path.join(json_directory, filename)\n",
    "        scrub_changed = False\n",
    "        with open(fpath) as f:\n",
    "            # json_decoded = json.load(json_file)\n",
    "            json_decoded = json.loads(f.read())\n",
    "            if 'content' in json_decoded and (not 'content_scrubbed' in json_decoded or do_scrub_rescrub):\n",
    "                json_decoded['content_scrubbed'] = scrub(json_decoded['content'])\n",
    "                scrub_changed = True\n",
    "            if do_scrub_delete_original_content and 'content_scrubbed' in json_decoded and 'content' in json_decoded:\n",
    "                json_decoded.pop('content', None)\n",
    "                scrub_changed = True\n",
    "        if scrub_changed:\n",
    "            with open(fpath, 'w') as json_file:\n",
    "                json.dump(json_decoded, json_file)\n",
    "            scrub_count += 1\n",
    "            ## progress indicator\n",
    "            if(scrub_count%100==0):\n",
    "                print('. ', end='')\n",
    "    print('Scrubbed ' + str(scrub_count) + ' files.')\n",
    "else:\n",
    "    print('Skipping scrub.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "**Deduplication is currently disabled, as it does not have an interface for large collections of JSON files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_dedupe = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DE-DUPLICATE\n",
    "\n",
    "## For help on script options:\n",
    "## %run scripts/deduplicate/corpus_compare.py -h \n",
    "\n",
    "if do_dedupe:\n",
    "\n",
    "    print(project_dir)\n",
    "    print(dedup_dir)\n",
    "    print(dedup_name)\n",
    "    \n",
    "    ## delete previous results\n",
    "    !rm -f {dedup_dir}/{dedup_output}.csv\n",
    "    !rm -f {dedup_dir}/{dedup_output}.log\n",
    "    !rm -f {dedup_output}.log\n",
    "\n",
    "    !mkdir -p {text_files_clean_dir}\n",
    "    %run {dedup_dir}/{dedup} -i caches/json/ -f *.json --threshold 0.8 -o {dedup_dir}/{dedup_name}.csv -l {dedup_dir}/{dedup_name}.log\n",
    "\n",
    "## --------------\n",
    "## FOR DockerFile\n",
    "## --------------\n",
    "## relies on sklearn\n",
    "## need to pip install or pip2 install or conda install scikit-learn?\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MERGE METADATA\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "if do_dedupe:\n",
    "    with open(project_dir + '/' + dedup_dir + '/' + dedup_name + '.csv','r') as fin:\n",
    "        cfin = csv.reader(fin)\n",
    "        # print(cfin, None)\n",
    "        next(cfin) # skip header\n",
    "        for row in cfin:\n",
    "            if os.path.isfile(row[5]):\n",
    "                print('Deleting: ' + row[5])\n",
    "                os.remove(row[5])\n",
    "            else:\n",
    "                print('Missing:  '+ row[5])\n",
    "    print('\\n-----\\nDuplicates deleted from:', dedup_dir + '/' + dedup_name + '.csv')\n",
    "\n",
    "else:\n",
    "    print('Skipping de-deuplicate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT: MALLET text files and DFR csv metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## CREATE METADATA FROM JSON FILES\n",
    "\n",
    "import json\n",
    "\n",
    "## Delete old metadata files\n",
    "!rm -fr {metadata_dir}\n",
    "!mkdir -p {metadata_dir}\n",
    "\n",
    "## Delete old text files\n",
    "!rm -fr {text_files_clean_dir}\n",
    "!mkdir -p {text_files_clean_dir}\n",
    "\n",
    "json_directory = 'caches/json/'\n",
    "\n",
    "## DEFINE METADATA STRINGCLEANER\n",
    "\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "def string_cleaner(unistr):\n",
    "    \"\"\"Returns string in unaccented form, printable characters only.\"\"\"\n",
    "    unaccented = unidecode.unidecode(unistr)\n",
    "    printonly = ''.join(filter(lambda x:x in string.printable, unaccented))\n",
    "    return printonly\n",
    "\n",
    "## MAP FIELDS FROM JSON TO DFRB METADATA\n",
    "\n",
    "## id, publication, pubdate, title, articlebody, author, docUrl, wordcount\n",
    "\n",
    "## idx       ->  id\n",
    "## title     ->  title\n",
    "##           ->  author\n",
    "## pub       ->  publication\n",
    "##           ->  docUrl\n",
    "## length    ->  wordcount\n",
    "## pub_date  ->  pubdate\n",
    "\n",
    "## content   ->  articlebody\n",
    "\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "metadata_csv_file = 'caches/metadata/metadata-dfrb.csv'\n",
    "\n",
    "# ## infieldnames provides names for the original column order\n",
    "# infieldnames = 'id', 'publication', 'pubdate', 'title', 'articlebody', 'pagerange', 'author', 'docUrl', 'wordcount'\n",
    "# ## outfieldnames re-orders that name list into a new column order\n",
    "# outfieldnames = 'id', 'title', 'author', 'publication', 'docUrl', 'wordcount', 'pubdate', 'pagerange'\n",
    "\n",
    "\n",
    "with open(metadata_csv_file, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "#   csvwriter.writerow(['id'] + ['publication'] + ['pubdate'] + ['title'] + ['articlebody'] + ['author'] + ['docUrl'] + ['wordcount'])\n",
    "#   csvwriter.writerow(['id'] + ['title'] + ['author'] + ['publication'] + ['docUrl'] + ['wordcount'] + ['pubdate'] + ['pagerange'])\n",
    "    csvwriter.writerow(['id'] + ['title'] + ['author'] + ['journaltitle'] + ['volume'] + ['issue'] + ['pubdate'] + ['pagerange'])\n",
    "\n",
    "    sorted_json = sorted(f for f in os.listdir(json_directory) if f.endswith(\".json\"))\n",
    "    \n",
    "    idx = 0\n",
    "    for filename in sorted_json:\n",
    "\n",
    "        # log: preview the first and last files only to prevent log overflow\n",
    "        if(idx<5 or idx > len(sorted_json)-5):\n",
    "            print(idx, ':', filename, '\\n')\n",
    "        if(idx==5 and len(sorted_json)>10):\n",
    "            print('...\\n')\n",
    "            \n",
    "        with open(os.path.join(json_directory, filename)) as f:\n",
    "            j = json.loads(f.read())\n",
    "            if not 'pagerange' in j:\n",
    "                j['pagerange'] = 'no-pg'\n",
    "            if not 'author' in j:\n",
    "                j['author'] = 'unknown'\n",
    "            if not 'volume'in j:\n",
    "                j['volume'] = 'no-vol'\n",
    "            if not 'issue' in j:\n",
    "                j['issue'] = 'no-issue'\n",
    "\n",
    "            # write article metadata to csv\n",
    "            # csvwriter.writerow([idx] + [j['title']] + [] + [j['pub']] + [] + [j['length']] + [j['pub_date']])\n",
    "            csvwriter.writerow(['json/' + filename] + [j['title']] + [j['author']] + [j['pub']] + [j['volume']] + [j['issue']] + [j['pub_date']] + [j['length']])\n",
    "\n",
    "            # name article body file\n",
    "            padded_id = str(idx).zfill(len(str(len(sorted_json))))\n",
    "            \n",
    "            # write article body file to txt\n",
    "            with open(project_dir+'/' + text_files_clean_dir + '/'+ padded_id + '_.txt', 'w') as outfile:\n",
    "                if 'content_scrubbed' in j:\n",
    "                    outfile.write(string_cleaner(j['content_scrubbed']))\n",
    "                else:\n",
    "                    outfile.write(string_cleaner(j['content']))\n",
    "\n",
    "            idx = idx+1\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CHECK METADATA\n",
    "\n",
    "!echo CHECK METADATA\n",
    "!echo\n",
    "!echo {metadata_dir} :\n",
    "!ls -1 {metadata_dir}\n",
    "!echo\n",
    "!echo {metadata_file_reorder} :\n",
    "!head -n 5 {metadata_file_reorder}\n",
    "!echo\n",
    "!echo CHECK TEXT FILES\n",
    "!echo\n",
    "!echo {text_files_clean_dir} :\n",
    "!ls -1 {text_files_clean_dir} | head\n",
    "!echo ...\n",
    "!ls -1 {text_files_clean_dir} | tail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL: build mallet topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## 1. run mallet -- import\n",
    "\n",
    "## build the mallet import command string\n",
    "mallet_import_args = '--input ' + project_dir + '/' + text_files_clean_dir + '/ ' \\\n",
    "  + '--output ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--keep-sequence ' \\\n",
    "  + '--remove-stopwords ' \\\n",
    "  + '--extra-stopwords ' + project_dir + '/' + stopwords_dir + '/' + stopwords_file + ' '\n",
    "mallet_import_command = 'mallet import-dir ' + mallet_import_args\n",
    "print(mallet_import_command+'\\n')\n",
    "\n",
    "## run mallet; capture and display output\n",
    "mout = !mallet import-dir {mallet_import_args}\n",
    "print('\\n'.join(mout)+'\\n')\n",
    "\n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel import done.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## 2. run mallet -- train\n",
    "\n",
    "## only generate diagnostics if feature available -- running on port 10000\n",
    "if(port==10000):\n",
    "    generate_diagnostics = True\n",
    "else:\n",
    "    generate_diagnostics = False\n",
    "    \n",
    "## build the mallet training command string\n",
    "mallet_train_args = '--input ' + project_dir + '/' + model_dir + '/' + model_file + ' ' \\\n",
    "  + '--num-topics ' + model_num_topics + ' ' \\\n",
    "  + '--optimize-interval 10 ' \\\n",
    "  + '--output-state ' + project_dir + '/' + model_dir + '/' + model_state + ' ' \\\n",
    "  + '--output-topic-keys ' + project_dir + '/' + model_dir + '/' + model_keys + ' ' \\\n",
    "  + '--output-doc-topics ' + project_dir + '/' + model_dir + '/' + model_composition + ' ' \\\n",
    "  + '--word-topic-counts-file ' + project_dir + '/' + model_dir + '/' + model_counts\n",
    "if use_random_seed == True:\n",
    "  mallet_train_args += ' --random-seed ' + model_random_seed\n",
    "if generate_diagnostics == True:\n",
    "  mallet_train_args += ' --diagnostics-file ' + project_dir + '/' + model_dir + '/diagnostics.xml'\n",
    "    \n",
    "mallet_train_command = 'mallet train-topics ' + mallet_train_args\n",
    "print(mallet_train_command+'\\n')\n",
    "\n",
    "print('\\nRunning:\\n')\n",
    "\n",
    "## run mallet\n",
    "!mallet train-topics {mallet_train_args}\n",
    "    \n",
    "print(os.listdir(project_dir + '/' + model_dir))\n",
    "\n",
    "print('\\n-----\\nModel training done.')\n",
    "\n",
    "if generate_diagnostics == True:\n",
    "    print('A diagnostics web page will be generated soon. This feature is not yet active. In the meantime, you can view the diagnostics.xml file in your model directory.')\n",
    "\n",
    "print('\\n\\n----------Time----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(generate_diagnostics):\n",
    "    print('View diagnostics.xml in Edit mode:')\n",
    "    diagnostics_edit_view = url_10000.replace('/notebooks/', '/edit/') + '/caches/model/diagnostics.xml'\n",
    "    from IPython.display import display, HTML\n",
    "    browser_link_html = HTML('<p><a href=\"' + diagnostics_edit_view + '\" target=\"_blank\"><strong>diagnostics.xml</strong></a></p>')\n",
    "    display(browser_link_html)\n",
    "else:\n",
    "    print('No diagnostics generated when run on 9999.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NEXT\n",
    "## Generate a link to the next notebook in the workflow\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "browser_link_html = HTML('<p>The topic model is built.</p><h2>Make a <a href=\"' + url_9999 + '/4_make_topic_browser.ipynb\" target=\"_blank\"><strong>dfrbrowser</strong> topic browser</a> (on 9999)</h2><p>...or...</p><h2>Make a <a href=\"' + url_10000 + '/6_browser_pyldavis.ipynb\" target=\"_blank\"><strong>pyLDAvis</strong> topic browser</a> (on 10000)</h2>')\n",
    "display(browser_link_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE DATA FOR DFR-BROWSER (python prepare-data) IN DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepare_dfr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "if prepare_dfr:\n",
    "\n",
    "    browser_meta_file_temp = 'caches/metadata/meta.temp.csv'\n",
    "    browser_meta_file = 'caches/metadata/meta.csv'\n",
    "\n",
    "    with open(metadata_csv_file, 'r') as csv_in:\n",
    "        csvreader = csv.reader(csv_in, delimiter=',')\n",
    "        next(csvreader)  # skip header row\n",
    "        with open(browser_meta_file_temp, 'w') as csv_out:\n",
    "            # enforce quoted fields\n",
    "            csvwriter = csv.writer(csv_out, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "            for row in csvreader:\n",
    "                csvwriter.writerow(row)\n",
    "            \n",
    "    with open(browser_meta_file_temp, 'r') as fin:\n",
    "        with open(browser_meta_file, 'w') as fout:\n",
    "            for line in fin:\n",
    "                fout.write(line.replace(',\"\",', ',NA,'))\n",
    "\n",
    "    !rm {browser_meta_file_temp}\n",
    "    !rm -r browser\n",
    "    # copy dfrbrowser template from scripts to project browser folder\n",
    "    !cp -r scripts/dfrbrowser-full/ browser/\n",
    "    !mv browser/js/dfb.min.js.custom browser/js/dfb.min.js\n",
    "    #rename customized minimized js file\n",
    "    !mv browser/js/dfb.min.js.custom browser/js/dfb.min.js\n",
    "    !mkdir browser/data/\n",
    "    \n",
    "    #dfbOutputDir = \"browser/data/\"\n",
    "    #!rm -r {dfbOutputDir}\n",
    "    #!mkdir -p {dfbOutputDir}\n",
    "    !scripts/dfrbrowser-full/bin/prepare-data convert-state caches/model/topic-state.gz --tw browser/data/tw.json --dt browser/data/dt.json.zip\n",
    "    !scripts/dfrbrowser-full/bin/prepare-data info-stub -o browser/data/info.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Topic Scaling Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scale_topics.py.\n",
    "\n",
    "Create a topic_scaled.csv file from the Mallet state file.\n",
    "\n",
    "Combines code by Jeri E. Wieringa (https://github.com/jerielizabeth/Gospel-of-Health-Notebooks/blob/master/blogPosts/pyLDAvis_and_Mallet.ipynb)\n",
    "to transform Mallet data for use with pyLDAvis and uses code derived\n",
    "from pyLDAvis to calculate topic coordinates using MDS.\n",
    "\n",
    "Configure the paths to the mallet state and topic_scaled files below.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=E1101\n",
    "# pylint: disable=W1201\n",
    "\n",
    "\n",
    "# Python imports\n",
    "import gzip\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "# Set fallback for MDS scaling\n",
    "try:\n",
    "    from sklearn.manifold import MDS, TSNE\n",
    "    sklearn_present = True\n",
    "except ImportError:\n",
    "    sklearn_present = False\n",
    "from past.builtins import basestring\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "# Configure the input and output file paths\n",
    "output_state_file = os.path.join(project_dir + '/caches/model', 'topic-state.gz')\n",
    "topic_scaled_file = os.path.join(project_dir + '/browser/data', 'topic_scaled.csv')\n",
    "\n",
    "\n",
    "def __num_dist_rows__(array, ndigits=2):\n",
    "    return array.shape[0] - int((pd.DataFrame(array).sum(axis=1) < 0.999).sum())\n",
    "\n",
    "\n",
    "class ValidationError(ValueError):\n",
    "    \"\"\"Handle validation errors.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def _input_check(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):\n",
    "    ttds = topic_term_dists.shape\n",
    "    dtds = doc_topic_dists.shape\n",
    "    errors = []\n",
    "    def err(msg):\n",
    "        \"\"\"Append error message.\"\"\"\n",
    "        errors.append(msg)\n",
    "\n",
    "    if dtds[1] != ttds[0]:\n",
    "        err('Number of rows of topic_term_dists does not match number of columns of doc_topic_dists; both should be equal to the number of topics in the model.')\n",
    "\n",
    "    if len(doc_lengths) != dtds[0]:\n",
    "        err('Length of doc_lengths not equal to the number of rows in doc_topic_dists; both should be equal to the number of documents in the data.')\n",
    "\n",
    "    W = len(vocab)\n",
    "    if ttds[1] != W:\n",
    "        err('Number of terms in vocabulary does not match the number of columns of topic_term_dists (where each row of topic_term_dists is a probability distribution of terms for a given topic).')\n",
    "    if len(term_frequency) != W:\n",
    "        err('Length of term_frequency not equal to the number of terms in the vocabulary (len of vocab).')\n",
    "\n",
    "    if __num_dist_rows__(topic_term_dists) != ttds[0]:\n",
    "        err('Not all rows (distributions) in topic_term_dists sum to 1.')\n",
    "\n",
    "    if __num_dist_rows__(doc_topic_dists) != dtds[0]:\n",
    "        err('Not all rows (distributions) in doc_topic_dists sum to 1.')\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        return errors\n",
    "\n",
    "\n",
    "def _input_validate(*args):\n",
    "    res = _input_check(*args)\n",
    "    if res:\n",
    "        raise ValidationError('\\n' + '\\n'.join([' * ' + s for s in res]))\n",
    "\n",
    "\n",
    "def _jensen_shannon(_P, _Q):\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "\n",
    "def _pcoa(pair_dists, n_components=2):\n",
    "    \"\"\"Principal Coordinate Analysis.\n",
    "\n",
    "    AKA Classical Multidimensional Scaling\n",
    "    code referenced from skbio.stats.ordination.pcoa\n",
    "    https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py\n",
    "    \"\"\"\n",
    "    # pairwise distance matrix is assumed symmetric\n",
    "    pair_dists = np.asarray(pair_dists, np.float64)\n",
    "\n",
    "    # perform SVD on double centred distance matrix\n",
    "    n = pair_dists.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = - H.dot(pair_dists ** 2).dot(H) / 2\n",
    "    eigvals, eigvecs = np.linalg.eig(B)\n",
    "\n",
    "    # Take first n_components of eigenvalues and eigenvectors\n",
    "    # sorted in decreasing order\n",
    "    ix = eigvals.argsort()[::-1][:n_components]\n",
    "    eigvals = eigvals[ix]\n",
    "    eigvecs = eigvecs[:, ix]\n",
    "\n",
    "    # replace any remaining negative eigenvalues and associated eigenvectors with zeroes\n",
    "    # at least 1 eigenvalue must be zero\n",
    "    eigvals[np.isclose(eigvals, 0)] = 0\n",
    "    if np.any(eigvals < 0):\n",
    "        ix_neg = eigvals < 0\n",
    "        eigvals[ix_neg] = np.zeros(eigvals[ix_neg].shape)\n",
    "        eigvecs[:, ix_neg] = np.zeros(eigvecs[:, ix_neg].shape)\n",
    "\n",
    "    return np.sqrt(eigvals) * eigvecs\n",
    "\n",
    "\n",
    "def js_PCoA(distributions):\n",
    "    \"\"\"Perform dimension reduction.\n",
    "\n",
    "    Works via Jensen-Shannon Divergence & Principal Coordinate Analysis\n",
    "    (aka Classical Multidimensional Scaling)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcoa : array, shape (`n_dists`, 2)\n",
    "\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    return _pcoa(dist_matrix)\n",
    "\n",
    "\n",
    "def js_MMDS(distributions, **kwargs):\n",
    "    \"\"\"Perform dimension reduction.\n",
    "\n",
    "    Works via Jensen-Shannon Divergence & Metric Multidimensional Scaling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.MDS()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mmds : array, shape (`n_dists`, 2)\n",
    "\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = MDS(n_components=2, random_state=0, dissimilarity='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def js_TSNE(distributions, **kwargs):\n",
    "    \"\"\"Perform dimension reduction.\n",
    "\n",
    "    Works via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distributions : array-like, shape (`n_dists`, `k`)\n",
    "        Matrix of distributions probabilities.\n",
    "\n",
    "    **kwargs : Keyword argument to be passed to `sklearn.manifold.TSNE()`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tsne : array, shape (`n_dists`, 2)\n",
    "\n",
    "    \"\"\"\n",
    "    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))\n",
    "    model = TSNE(n_components=2, random_state=0, metric='precomputed', **kwargs)\n",
    "    return model.fit_transform(dist_matrix)\n",
    "\n",
    "\n",
    "def _df_with_names(data, index_name, columns_name):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def _series_with_name(data, name):\n",
    "    if isinstance(data, pd.Series):\n",
    "        data.name = name\n",
    "        # ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)\n",
    "\n",
    "\n",
    "def _topic_coordinates(mds, topic_term_dists, topic_proportion):\n",
    "    K = topic_term_dists.shape[0]\n",
    "    mds_res = mds(topic_term_dists)\n",
    "    assert mds_res.shape == (K, 2)\n",
    "    mds_df = pd.DataFrame({'x': mds_res[:, 0], 'y': mds_res[:, 1], 'topics': range(1, K + 1), \\\n",
    "                            'cluster': 1, 'Freq': topic_proportion * 100})\n",
    "    # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26\n",
    "    return mds_df\n",
    "\n",
    "\n",
    "def get_topic_coordinates(topic_term_dists, doc_topic_dists, doc_lengths, \\\n",
    "            vocab, term_frequency, mds=js_PCoA, sort_topics=True):\n",
    "    \"\"\"Transform the topic model distributions and related corpus.\n",
    "\n",
    "    Creates the data structures needed for topic bubbles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic_term_dists : array-like, shape (`n_topics`, `n_terms`)\n",
    "        Matrix of topic-term probabilities. Where `n_terms`\n",
    "        is `len(vocab)`.\n",
    "    doc_topic_dists : array-like, shape (`n_docs`, `n_topics`)\n",
    "        Matrix of document-topic probabilities.\n",
    "    doc_lengths : array-like, shape `n_docs`\n",
    "        The length of each document, i.e. the number of words\n",
    "        in each document. The order of the numbers should be\n",
    "        consistent with the ordering of the docs in `doc_topic_dists`.\n",
    "    vocab : array-like, shape `n_terms`\n",
    "        List of all the words in the corpus used to train the model.\n",
    "    term_frequency : array-like, shape `n_terms`\n",
    "        The count of each particular term over the entire corpus.\n",
    "        The ordering of these counts should correspond with\n",
    "        `vocab` and `topic_term_dists`.\n",
    "    mds : function or a string representation of function\n",
    "        A function that takes `topic_term_dists` as an input and\n",
    "        outputs a `n_topics` by `2`  distance matrix. The output\n",
    "        approximates the distance between topics. See :func:`js_PCoA`\n",
    "        for details on the default function. A string representation\n",
    "        currently accepts `pcoa` (or upper case variant), `mmds`\n",
    "        (or upper case variant) and `tsne` (or upper case variant),\n",
    "        if `sklearn` package is installed for the latter two.\n",
    "    sort_topics : sort topics by topic proportion (percentage of\n",
    "        tokens covered). Set to False to to keep original topic order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    topic_coordinates : A pandas dataframe containing\n",
    "        scaled x and y coordinates.\n",
    "\n",
    "    \"\"\"\n",
    "    # parse mds\n",
    "    if isinstance(mds, basestring):\n",
    "        mds = mds.lower()\n",
    "        if mds == 'pcoa':\n",
    "            mds = js_PCoA\n",
    "        elif mds in ('mmds', 'tsne'):\n",
    "            if sklearn_present:\n",
    "                mds_opts = {'mmds': js_MMDS, 'tsne': js_TSNE}\n",
    "                mds = mds_opts[mds]\n",
    "            else:\n",
    "                logging.warning('sklearn not present, switch to PCoA')\n",
    "                mds = js_PCoA\n",
    "        else:\n",
    "            logging.warning('Unknown mds `%s`, switch to PCoA' % mds)\n",
    "            mds = js_PCoA\n",
    "\n",
    "    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n",
    "    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n",
    "    term_frequency = _series_with_name(term_frequency, 'term_frequency')\n",
    "    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n",
    "    vocab = _series_with_name(vocab, 'vocab')\n",
    "    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\n",
    "\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    if sort_topics:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    else:\n",
    "        topic_proportion = (topic_freq / topic_freq.sum())\n",
    "\n",
    "    topic_order = topic_proportion.index\n",
    "    topic_term_dists = topic_term_dists.iloc[topic_order]\n",
    "\n",
    "    scaled_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)\n",
    "\n",
    "    return scaled_coordinates\n",
    "\n",
    "\n",
    "def extract_params(statefile):\n",
    "    \"\"\"Extract the alpha and beta values from the statefile.\n",
    "\n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET.\n",
    "    Returns:\n",
    "        tuple: alpha (list), beta\n",
    "\n",
    "    \"\"\"\n",
    "    with gzip.open(statefile, 'r') as state:\n",
    "        params = [x.decode('utf8').strip() for x in state.readlines()[1:3]]\n",
    "    return (list(params[0].split(\":\")[1].split(\" \")), float(params[1].split(\":\")[1]))\n",
    "\n",
    "\n",
    "def state_to_df(statefile):\n",
    "    \"\"\"Transform state file into pandas dataframe.\n",
    "\n",
    "    The MALLET statefile is tab-separated, and the first two rows contain the alpha and beta hypterparamters.\n",
    "\n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET.\n",
    "    Returns:\n",
    "        datframe: topic assignment for each token in each document of the model\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.read_csv(statefile,\n",
    "                        compression='gzip',\n",
    "                        sep=' ',\n",
    "                        skiprows=[1, 2]\n",
    "                        )\n",
    "\n",
    "\n",
    "def pivot_and_smooth(df, smooth_value, rows_variable, cols_variable, values_variable):\n",
    "    \"\"\"Turn the pandas dataframe into a data matrix.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): aggregated dataframe\n",
    "        smooth_value (float): value to add to the matrix to account for the priors\n",
    "        rows_variable (str): name of dataframe column to use as the rows in the matrix\n",
    "        cols_variable (str): name of dataframe column to use as the columns in the matrix\n",
    "        values_variable(str): name of the dataframe column to use as the values in the matrix\n",
    "    Returns:\n",
    "        dataframe: pandas matrix that has been normalized on the rows.\n",
    "\n",
    "    \"\"\"\n",
    "    matrix = df.pivot(index=rows_variable, columns=cols_variable, values=values_variable).fillna(value=0)\n",
    "    matrix = matrix.values + smooth_value\n",
    "\n",
    "    normed = sklearn.preprocessing.normalize(matrix, norm='l1', axis=1)\n",
    "\n",
    "    return pd.DataFrame(normed)\n",
    "\n",
    "\n",
    "def convert_mallet_data(state_file):\n",
    "    \"\"\"Convert Mallet data to a structure compatible with pyLDAvis.\n",
    "\n",
    "    Args:\n",
    "        output_state_file (string): Mallet state file\n",
    "\n",
    "    Returns:\n",
    "        data: dict containing pandas dataframes for the pyLDAvis prepare method.\n",
    "\n",
    "    \"\"\"\n",
    "    params = extract_params(state_file)\n",
    "    alpha = [float(x) for x in params[0][1:]]\n",
    "    beta = params[1]\n",
    "    df = state_to_df(state_file)\n",
    "    # Ensure that NaN is a string\n",
    "    df['type'] = df.type.astype(str)\n",
    "    # Get document lengths from statefile\n",
    "    docs = df.groupby('#doc')['type'].count().reset_index(name='doc_length')\n",
    "    # Get vocab and term frequencies from statefile\n",
    "    vocab = df['type'].value_counts().reset_index()\n",
    "    vocab.columns = ['type', 'term_freq']\n",
    "    vocab = vocab.sort_values(by='type', ascending=True)\n",
    "    phi_df = df.groupby(['topic', 'type'])['type'].count().reset_index(name='token_count')\n",
    "    phi_df = phi_df.sort_values(by='type', ascending=True)\n",
    "    phi = pivot_and_smooth(phi_df, beta, 'topic', 'type', 'token_count')\n",
    "    theta_df = df.groupby(['#doc', 'topic'])['topic'].count().reset_index(name='topic_count')\n",
    "    theta = pivot_and_smooth(theta_df, alpha, '#doc', 'topic', 'topic_count')\n",
    "    data = {'topic_term_dists': phi,\n",
    "            'doc_topic_dists': theta,\n",
    "            'doc_lengths': list(docs['doc_length']),\n",
    "            'vocab': list(vocab['type']),\n",
    "            'term_frequency': list(vocab['term_freq'])\n",
    "        }\n",
    "    return data\n",
    "\n",
    "# Convert the Mallet data and export the topic_scale.csv file\n",
    "converted_data = convert_mallet_data(output_state_file)\n",
    "topic_coordinates = get_topic_coordinates(**converted_data)\n",
    "topic_coordinates.to_csv(topic_scaled_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## move metadata-dfrb to browser/data, zip up and rename, delete meta.csv copy\n",
    "!rm browser/data/meta.csv.zip\n",
    "!cp {browser_meta_file} browser/data/\n",
    "!zip -j browser/data/meta.csv.zip browser/data/meta.csv\n",
    "!rm browser/data/meta.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## copy json cache into local browser for links\n",
    "\n",
    "# R code:\n",
    "# system(paste0(\"if [ -d \\\"browser/json\\\" ]; then rm -rf \\\"browser/json\\\"; fi\"))\n",
    "# system(paste0(\"mkdir -p browser/json && cp -rf caches/json browser/\"))\n",
    "# system(paste0(\"chmod\",\" \",\"755\",\" \",\"browser/json/*.json\"))\n",
    "\n",
    "if prepare_dfr:\n",
    "    !if [ -d browser/json ]; then rm -rf browser/json; fi\n",
    "    !mkdir -p browser/json && cp -rf caches/json browser/\n",
    "    !find browser/json -type f -print0 | xargs -0 chmod 755\n",
    "    #!chmod 755 browser/json/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tweak default index.html to link to JSON, not JSTOR\n",
    "\n",
    "# R code:\n",
    "# tx  <- readLines(\"browser/index.html\")\n",
    "# tx2  <- gsub(pattern = \"on JSTOR\", replace = \"JSON\", x = tx)\n",
    "# writeLines(tx2, con=\"browser/index.html\")\n",
    "if prepare_dfr:\n",
    "    fpath_html = \"browser/index.html\"\n",
    "    with open(fpath_html, 'r') as file:\n",
    "        filedata = file.read()\n",
    "    filedata = filedata.replace('on JSTOR', 'JSON')\n",
    "    with open(fpath_html, 'w') as file:\n",
    "        file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate an HTML menu with live browsing and download links\n",
    "## based on the current working directory.\n",
    "\n",
    "# R code:\n",
    "# project_name <- basename(getwd())\n",
    "# project_reldir <- strsplit( getwd(), \"/write/\" )[[1]][2]\n",
    "\n",
    "# IRdisplay::display_html(data=paste(\n",
    "    # \"<h2>Live</h2>\",\n",
    "    # \"<p>To view the browser live:</p>\",\n",
    "    # \"  <ul>\",\n",
    "    # paste(\"    <li><a href='http://harbor.english.ucsb.edu:10001/\", project_reldir, \"/\", dfbOutputDir, \"/' target='_blank'>Browser LIVE</a></li>\", sep = \"\"),\n",
    "    #\"  </ul>\"))\n",
    "import os\n",
    "    \n",
    "if prepare_dfr:\n",
    "    project_name = os.path.basename(project_dir)\n",
    "    project_reldir = project_dir.split(\"/write/\")[1]\n",
    "    project_link = \"http://harbor.english.ucsb.edu:10001/\" + project_reldir + \"/browser/\"\n",
    "    ## Can't get HTML display to work, so hack:\n",
    "    print(\"To view the browser live: \" + project_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a zipped copy of the browser for export\n",
    "\n",
    "zip_export = FALSE\n",
    "\n",
    "if (zip_export) {\n",
    "    zip(dfbZipFile, paste0(dfbOutputDir,\"/\"))\n",
    "    IRdisplay::display_html(data=paste(\n",
    "    \"<h2>Download</h2>\",\n",
    "    \"<p>To download and view the browser through a webserver hosted on your local machine:</p>\",\n",
    "    \"  <ol>\",\n",
    "    \"    <li><a href='\",dfbZipFile,\"' target='new'>Download browser.zip</a></li>\",\n",
    "    \"    <li>Unzip browser.zip</li>\",\n",
    "    \"    <li>Open a shell/terminal, and navigate to the browser directory</li>\",\n",
    "    \"    <li>On Linux / OSX, launch local webserver by running:<br><code>./bin/server</code></li>\",\n",
    "    \"    <li>View from your local webserver: <a href='http://localhost:8888/' target='_blank'>http://localhost:8888/</a></li>\",\n",
    "    \"  </ol>\"))\n",
    "} else {\n",
    "    IRdisplay::display_html(data=paste(\"<p>Zip export disabled.</p>\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIME elapsed if Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_time = Sys.time()\n",
    "\n",
    "cat(\"start: \")\n",
    "print(start_time)\n",
    "cat(\"stop:  \")\n",
    "print(stop_time)\n",
    "\n",
    "elapsed_time = stop_time - start_time\n",
    "print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
